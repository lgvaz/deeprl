{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-20 00:46:25,644] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "n_features = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Approximator():\n",
    "    ''' Creates an ANN to aporoximate Q values '''\n",
    "    \n",
    "    def __init__(self, n_features, n_actions, hidden_size, learning_rate=1e-2,\n",
    "                 clip_grads=False, scope='main', summary_name=None):\n",
    "        self.summary_writer = None \n",
    "        self.scope = scope\n",
    "        with tf.variable_scope(self.scope):\n",
    "            self.build(n_features, n_actions, hidden_size, learning_rate, clip_grads)                   \n",
    "            if summary_name:\n",
    "                # Make summary dir        \n",
    "                if not os.path.exists('summary'):\n",
    "                    os.mkdir('summary')\n",
    "                self.summary_writer = tf.train.SummaryWriter('summary/q_learning/{}'.format(summary_name))                     \n",
    "        \n",
    "    def build(self, n_features, n_actions, hidden_size, learning_rate, clip_grads):        \n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(name='input', shape=[None, n_features], dtype=tf.float32)\n",
    "        self.Y = tf.placeholder(name='target', shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(name='chosen_action', shape=[None], dtype=tf.int32)\n",
    "        # Stores the time step\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)        \n",
    "\n",
    "        # Network body\n",
    "#         with slim.arg_scope([slim.fully_connected], weights_initializer=tf.truncated_normal_initializer(stddev=0.01)):\n",
    "        self.hidden = slim.fully_connected(inputs=self.X, num_outputs=hidden_size, activation_fn=tf.nn.sigmoid)\n",
    "#         self.hidden = slim.fully_connected(inputs=self.hidden, num_outputs=hidden_size, activation_fn=tf.nn.relu)\n",
    "        self.Q = slim.fully_connected(inputs=self.hidden, num_outputs=n_actions, activation_fn=None)\n",
    "               \n",
    "        if self.scope == 'main':\n",
    "            # Loss\n",
    "            # Get the action value only for the chosen action\n",
    "            batch_size = tf.shape(self.X)[0]\n",
    "            action_ids = tf.range(batch_size) * tf.shape(self.Q)[1] + self.action\n",
    "            self.actions_value = tf.gather(tf.reshape(self.Q, [-1]), action_ids)\n",
    "            # Use the mean squared error\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.Y, self.actions_value))\n",
    "            # Compute and apply gradients\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            self.grads_and_vars = self.opt.compute_gradients(self.loss)\n",
    "            self.clipped_grads = [(tf.clip_by_value(grad, -1, 1), var) for grad, var\n",
    "                                 in self.grads_and_vars]\n",
    "            self.train_op = self.opt.apply_gradients(self.clipped_grads, self.global_step)\n",
    "        # Target network dont need to compute loss\n",
    "        else:\n",
    "            self.train_op = None\n",
    "            \n",
    "        # Tensorboard summaries\n",
    "        self.summaries = tf.merge_summary([\n",
    "#                 tf.histogram_summary('q_values', self.Q),\n",
    "                tf.scalar_summary('max_q_values', tf.reduce_max(self.Q)),\n",
    "#                 tf.scalar_summary('loss', self.loss),               \n",
    "            ])\n",
    "\n",
    "    def predict(self, sess, state):\n",
    "        ''' Calculate Q value '''\n",
    "        return sess.run(self.Q, feed_dict={self.X:state})  \n",
    "\n",
    "    def update(self, sess, state, action, target):\n",
    "        ''' Update weights and write summary '''\n",
    "        feed_dict = {self.X:state, self.action:action, self.Y:target}\n",
    "        summary, _ = sess.run([self.summaries, self.train_op], feed_dict=feed_dict)  \n",
    "        step = sess.run(self.global_step)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summary, step)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Target_Net:\n",
    "    ''' Creates the target network '''\n",
    "    def __init__(self, n_features, n_actions, hidden_size, learning_rate=1e-2, clip_grads=False):\n",
    "        self.build(n_features, n_actions, hidden_size, learning_rate, clip_grads)\n",
    "        \n",
    "    def build(self, n_features, n_actions, hidden_size, learning_rate, clip_grads):\n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(name='input', shape=[None, n_features], dtype=tf.float32)\n",
    "        self.Y = tf.placeholder(name='target', shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(name='chosen_action', shape=[None], dtype=tf.int32)\n",
    "        # Stores the time step\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)        \n",
    "\n",
    "        # Network body\n",
    "        self.hidden = slim.fully_connected(inputs=self.X, num_outputs=hidden_size, activation_fn=tf.nn.relu)\n",
    "        self.hidden = slim.fully_connected(inputs=self.hidden, num_outputs=hidden_size, activation_fn=tf.nn.relu)\n",
    "        self.Q = slim.fully_connected(inputs=self.hidden, num_outputs=n_actions, activation_fn=None)\n",
    "        \n",
    "    def predict(self, sess, state):\n",
    "        ''' Calculate Q value '''\n",
    "        return sess.run(self.Q, feed_dict={self.X:state})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Replay_Memory():\n",
    "    ''' Stores states and transitions used to train the network '''\n",
    "    def __init__(self, limit=200000):\n",
    "        self.memory = []\n",
    "        self.limit = limit\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        ''' Adds a new experience to memory '''\n",
    "        if len(self.memory) == self.limit:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ''' Returns random experiences '''\n",
    "        return random.sample(self.memory, batch_size)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def update_target_op(tvars, tau):\n",
    "    ''' \n",
    "    Update target network to be \"tau\" times closer to the learning one\n",
    "    The target network has to be defined last\n",
    "    '''\n",
    "    n_vars = len(tvars)\n",
    "    target_start = n_vars//2\n",
    "    op_holder = []\n",
    "    for idx, var in enumerate(tvars[0:target_start]):\n",
    "        op_holder.append(tvars[target_start + idx].assign(var.value() * tau)\n",
    "                        + (1 - tau) * tvars[target_start + idx].value())\n",
    "    return op_holder    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def update_target(sess, op_holder):\n",
    "    ''' Runs the operation of updating target network '''\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target_op(main, target, tau):\n",
    "    ''' \n",
    "    Update target network to be \"tau\" times closer to the learning one\n",
    "    The target network has to be defined last\n",
    "    '''\n",
    "    # Get trainable variables from main and target network\n",
    "    main_vars = [var for var in tf.trainable_variables() if var.name.startswith(main.scope)]\n",
    "    target_vars = [var for var in tf.trainable_variables() if var.name.startswith(target.scope)]\n",
    "    # Get the operations that update target network\n",
    "    op_holder = []\n",
    "    for main_var, target_var in zip(main_vars, target_vars):\n",
    "        op_holder.append(target_var.assign(main_var.value() * tau)\n",
    "                        + (1 - tau) * target_var.value())\n",
    "    \n",
    "    def update_target(sess):\n",
    "        sess.run(op_holder)\n",
    "        \n",
    "    return update_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epolicy(Q, epsilon):\n",
    "    ''' Make an epsilon greedy choice '''   \n",
    "    A = (np.ones(n_actions) * epsilon) / n_actions\n",
    "    best_action = np.argmax(np.squeeze(Q))\n",
    "    A[best_action] += (1 - epsilon)    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_updates(n_updates=100, batch_size=100, learning_rate=3e-4):\n",
    "    ''' Test if weights updates are affecting mostly the chosen action '''\n",
    "    tf.reset_default_graph()\n",
    "    test = Approximator(n_features, n_actions, 10, learning_rate)\n",
    "    states = [env.observation_space.sample() for _ in range(batch_size)]\n",
    "    targets = 10 * np.ones(batch_size)\n",
    "    for action in range(n_actions):\n",
    "        Q_diff = []\n",
    "        actions = action * np.ones(batch_size)\n",
    "        # Reinitialize model for each action because ADAM optimizer builds momentum\n",
    "        with tf.Session() as sess:\n",
    "            tf.initialize_all_variables().run()\n",
    "            for _ in range(n_updates):\n",
    "                Q_old = np.mean(test.predict(sess, states), axis=0)\n",
    "                test.update(sess, states, actions, targets)\n",
    "                Q_new = np.mean(test.predict(sess, states), axis=0)\n",
    "                Q_diff.append(Q_new - Q_old)\n",
    "            print('Action {}: {}'.format(action, np.sum(Q_diff, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolve(x, window_size):\n",
    "    ''' Take the mean of array over window '''\n",
    "    weigths = np.repeat(1.0, window_size) / window_size\n",
    "    return np.convolve(x, weigths, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0: [ 0.18059599  0.        ]\n",
      "Action 1: [ 0.          0.18839598]\n"
     ]
    }
   ],
   "source": [
    "test_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(n_episodes, learning_rate, discount_factor=0.99, epsilon_max=1, epsilon_min=0.01,\n",
    "          epsilon_step=2e-3, tau=1e-3, hidden_size=16, clip_grads=False,\n",
    "          batch_size=32, min_replays=10000, update_step=1000, summary=None):\n",
    "    \n",
    "    # Create the network to approximate Q function\n",
    "    tf.reset_default_graph()\n",
    "    Q_main = Approximator(n_features, n_actions, hidden_size, learning_rate,\n",
    "                          clip_grads=clip_grads, summary_name=summary)\n",
    "    Q_target = Approximator(n_features, n_actions, hidden_size, learning_rate,\n",
    "                            clip_grads=clip_grads, scope='target')\n",
    "    # Create operation that update target network\n",
    "#     tvars = tf.trainable_variables()\n",
    "    update_op = update_target_op(Q_main, Q_target, tau)\n",
    "    \n",
    "    # Store episodes length\n",
    "    ep_length = []\n",
    "    \n",
    "    # Create experience replay\n",
    "    replays = Replay_Memory()\n",
    "    state = env.reset()\n",
    "    # Fill replay memory with random actions\n",
    "    action_probs = np.ones(n_actions) / n_actions\n",
    "    for _ in range(min_replays):\n",
    "        action = np.random.choice(np.arange(n_actions), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Record experience\n",
    "        replays.add(state, action, reward, next_state, done)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        update_op(sess)\n",
    "\n",
    "        for i_episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            # Exponentially decay epsilon\n",
    "            epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_step * i_episode)            \n",
    "\n",
    "            # Repeat until episode is finished\n",
    "            for t in itertools.count():\n",
    "                # Choose an action\n",
    "                Q = Q_main.predict(sess, [state])\n",
    "                action_probs = epolicy(Q, epsilon)   \n",
    "                action = np.random.choice(np.arange(n_actions), p=action_probs)\n",
    "                # Do the action\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Record experience\n",
    "                replays.add(state, action, reward, next_state, done)\n",
    "\n",
    "                # Get replays to train on\n",
    "                b_state, b_action, b_reward, b_next_state, b_done = map(np.array, zip(*replays.sample(batch_size)))\n",
    "                # Generate td_targets (target is only the reward if done)\n",
    "                Q_next = Q_target.predict(sess, b_next_state)\n",
    "                Q_next_max = np.max(Q_next, axis=1)\n",
    "                b_td_targets = b_reward + np.bitwise_xor(b_done, 1) * (discount_factor * Q_next_max)\n",
    "                # Update weights\n",
    "                Q_main.update(sess, b_state, b_action, b_td_targets)\n",
    "\n",
    "                # Update state        \n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "                \n",
    "            # Update target network\n",
    "            if i_episode % update_step == 0:\n",
    "                update_op(sess)\n",
    "            \n",
    "            # Add episode length to tensorboard\n",
    "            if summary is not None:                \n",
    "                episode_summary = tf.Summary()\n",
    "                episode_summary.value.add(simple_value=t, tag='Episode length')\n",
    "                Q_main.summary_writer.add_summary(episode_summary, i_episode)\n",
    "\n",
    "            # Store and print information\n",
    "            ep_length.append(t)\n",
    "            print('\\rEpisode {}/{} | Length: {}'.format(i_episode+1, n_episodes, t), end='', flush=True)\n",
    "            \n",
    "    return ep_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1446/10000 | Length: 28"
     ]
    }
   ],
   "source": [
    "ep_length = train(n_episodes=10000, learning_rate=3e-4, clip_grads=False, tau=1, update_step=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_length_clipped = train(n_episodes=10000, learning_rate=3e-4, clip_grads=True, tau=1, update_step=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot episodes length smoothed over a window size of 10\n",
    "plt.plot(convolve(ep_length, 10), label='unclipped grads')\n",
    "plt.plot(convolve(ep_length_clipped, 10), label='clipped grads')\n",
    "plt.title('Average episode length')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('length')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
