{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-17 18:33:13,929] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "n_features = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5f64bc7b18e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mApproximator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m''' Creates an ANN to aporoximate Q values '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'main'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5f64bc7b18e8>\u001b[0m in \u001b[0;36mApproximator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;34m''' Update weights and write summary '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "class Approximator():\n",
    "    ''' Creates an ANN to aporoximate Q values '''\n",
    "    \n",
    "    def __init__(self, n_features, n_actions, hidden_size, learning_rate=1e-2, clip_grads=False, scope='main', summary_name=None):\n",
    "        self.build(n_features, n_actions, hidden_size, learning_rate, clip_grads, scope)\n",
    "        self.summary_writer = None\n",
    "        self.scope = scope\n",
    "        if summary_name:\n",
    "            # Make summary dir        \n",
    "            if not os.path.exists('summary'):\n",
    "                os.mkdir('summary')\n",
    "            self.summary_writer = tf.train.SummaryWriter('summary/q_learning/{}'.format(summary_name))                     \n",
    "        \n",
    "    def build(self, n_features, n_actions, hidden_size, learning_rate, clip_grads):        \n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(name='input', shape=[None, n_features], dtype=tf.float32)\n",
    "        self.Y = tf.placeholder(name='target', shape=[None], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(name='chosen_action', shape=[None], dtype=tf.int32)\n",
    "        # Stores the time step\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)        \n",
    "\n",
    "        # Network body\n",
    "        self.hidden = slim.fully_connected(inputs=self.X, num_outputs=hidden_size, activation_fn=tf.nn.sigmoid)\n",
    "        self.Q = slim.fully_connected(inputs=self.hidden, num_outputs=n_actions, activation_fn=None)\n",
    "        \n",
    "        # Only compute the loss for the main network\n",
    "        if self.scope == 'main':\n",
    "            # Loss\n",
    "            # Get the action value only for the chosen action\n",
    "            batch_size = tf.shape(self.X)[0]\n",
    "            action_ids = tf.range(batch_size) * tf.shape(self.Q)[1] + self.action\n",
    "            self.actions_value = tf.gather(tf.reshape(self.Q, [-1]), action_ids)\n",
    "            # Use the mean squared error\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.Y, self.actions_value))\n",
    "            # Compute and apply gradients\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if clip_grads == True:                    \n",
    "                self.grads_and_vars = self.opt.compute_gradients(self.loss)\n",
    "                self.clipped_grads = [(tf.clip_by_value(grad, -1, 1), var) for grad, var in self.grads_and_vars]\n",
    "                self.train_op = self.opt.apply_gradients(self.grads_and_vars, self.global_step)\n",
    "            else:\n",
    "                self.train_op = self.opt.minimize(self.loss, self.global_step)\n",
    "\n",
    "        # Tensorboard summaries\n",
    "        self.summaries = tf.merge_summary([\n",
    "                tf.histogram_summary('q_values', self.Q),\n",
    "                tf.scalar_summary('max_q_values', tf.reduce_max(self.Q)),\n",
    "                tf.scalar_summary('loss', self.loss),               \n",
    "            ])\n",
    "        \n",
    "    def predict(self, sess, state):\n",
    "        ''' Calculate Q value '''\n",
    "        return sess.run(self.Q, feed_dict={self.X:state})  \n",
    "    \n",
    "    if self.scope == 'main':\n",
    "        def update(self, sess, state, action, target):\n",
    "            ''' Update weights and write summary '''\n",
    "            feed_dict = {self.X:state, self.action:action, self.Y:target}\n",
    "            summary, _ = sess.run([self.summaries, self.train_op], feed_dict=feed_dict)  \n",
    "            step = sess.run(self.global_step)\n",
    "            if self.summary_writer:\n",
    "                self.summary_writer.add_summary(summary, step)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Replay_Memory():\n",
    "    ''' Stores states and transitions used to train the network '''\n",
    "    def __init__(self, limit=200000):\n",
    "        self.memory = []\n",
    "        self.limit = limit\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        ''' Adds a new experience to memory '''\n",
    "        if len(self.memory) == self.limit:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ''' Returns random experiences '''\n",
    "        return random.sample(self.memory, batch_size)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target_op(tvars, tau):\n",
    "    ''' \n",
    "    Update target network to be \"tau\" times closer to the learning one\n",
    "    The target network has to be defined last\n",
    "    '''\n",
    "    n_vars = len(tvars)\n",
    "    target_start = n_vars//2\n",
    "    op_holder = []\n",
    "    for idx, var in enumerate(tvars[0:target_start]):\n",
    "        op_holder.append(tvars[target_start + idx].assign(var.value() * tau)\n",
    "                        + (1 - tau) * tvars[target_start + idx].value())\n",
    "    return op_holder    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_target(sess, op_holder):\n",
    "    ''' Runs the operation of updating target network '''\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epolicy(Q, epsilon):\n",
    "    ''' Make an epsilon greedy choice '''   \n",
    "    A = (np.ones(n_actions) * epsilon) / n_actions\n",
    "    best_action = np.argmax(np.squeeze(Q))\n",
    "    A[best_action] += (1 - epsilon)    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_updates(n_updates=100, batch_size=100, learning_rate=1e-2):\n",
    "    ''' Test if weights updates are affecting mostly the chosen action '''\n",
    "    tf.reset_default_graph()\n",
    "    test = Approximator(n_features, n_actions, 10, learning_rate)\n",
    "    states = [env.observation_space.sample() for _ in range(batch_size)]\n",
    "    targets = 10 * np.ones(batch_size)\n",
    "    for action in range(n_actions):\n",
    "        Q_diff = []\n",
    "        actions = action * np.ones(batch_size)\n",
    "        # Reinitialize model for each action because ADAM optimizer builds momentum\n",
    "        with tf.Session() as sess:\n",
    "            tf.initialize_all_variables().run()\n",
    "            for _ in range(n_updates):\n",
    "                Q_old = np.mean(test.predict(sess, states), axis=0)\n",
    "                test.update(sess, states, actions, targets)\n",
    "                Q_new = np.mean(test.predict(sess, states), axis=0)\n",
    "                Q_diff.append(Q_new - Q_old)\n",
    "            print('Action {}: {}'.format(action, np.sum(Q_diff, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(n_episodes, learning_rate, discount_factor=0.99, epsilon_max=1, epsilon_min=0.01, tau=1e-3,\n",
    "          hidden_size=16, clip_grads=False, batch_size=32, min_replays=10000, summary=None):\n",
    "    \n",
    "    # Create the network to approximate Q function\n",
    "    tf.reset_default_graph()\n",
    "    Q_main = Approximator(n_features, n_actions, hidden_size, learning_rate,\n",
    "                          clip_grads=clip_grads, summary_name=summary, scope='main')\n",
    "    Q_target = Approximator(n_features, n_actions, hidden_size, learning_rate,\n",
    "                            clip_grads=clip_grads, summary_name=summary, scope='target')\n",
    "    # Create operation that update target network\n",
    "    tvars = tf.trainable_variables()\n",
    "    update_op = update_target_op(tvars, tau)\n",
    "    \n",
    "    # Store episodes length\n",
    "    ep_length = []\n",
    "    \n",
    "    # Create experience replay\n",
    "    replays = Replay_Memory()\n",
    "    state = env.reset()\n",
    "    # Fill replay memory with random actions\n",
    "    action_probs = np.ones(n_actions) / n_actions\n",
    "    for _ in range(min_replays):\n",
    "        action = np.random.choice(np.arange(n_actions), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Record experience\n",
    "        replays.add(state, action, reward, next_state, done)\n",
    "        # Update state\n",
    "        state = next_state\n",
    "        if done:\n",
    "            state = env.reset()    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        update_target(sess, update_op)\n",
    "\n",
    "        for i_episode in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            # Exponentially decay epsilon\n",
    "            epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-5e-4 * i_episode)            \n",
    "\n",
    "            # Repeat until episode is finished\n",
    "            for t in itertools.count():\n",
    "                # Choose an action\n",
    "                Q = Q_main.predict(sess, [state])\n",
    "                action_probs = epolicy(Q, epsilon)   \n",
    "                action = np.random.choice(np.arange(n_actions), p=action_probs)\n",
    "                # Do the action\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # Record experience\n",
    "                replays.add(state, action, reward, next_state, done)\n",
    "\n",
    "                # Get replays to train on\n",
    "                b_state, b_action, b_reward, b_next_state, b_done = map(np.array, zip(*replays.sample(16)))\n",
    "                # Generate td_targets (target is only the reward if done)\n",
    "                Q_next = Q_target.predict(sess, b_next_state)\n",
    "                Q_next_max = np.max(Q_next, axis=1)\n",
    "                b_td_targets = b_reward + np.bitwise_xor(b_done, 1) * (discount_factor * Q_next_max)\n",
    "                # Update weights\n",
    "                Q_main.update(sess, b_state, b_action, b_td_targets)\n",
    "                # Update target network\n",
    "                update_target(sess, update_op)\n",
    "\n",
    "                # Update state        \n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "            \n",
    "            # Add episode length to tensorboard\n",
    "            if summary is not None:                \n",
    "                episode_summary = tf.Summary()\n",
    "                episode_summary.value.add(simple_value=t, tag='Episode length')\n",
    "                Q_main.summary_writer.add_summary(episode_summary, i_episode)\n",
    "\n",
    "            # Store and print information\n",
    "            ep_length.append(t)\n",
    "            print('\\rEpisode {}/{} | Length: {}'.format(i_episode+1, n_episodes, t), end='', flush=True)\n",
    "            \n",
    "    return ep_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ep_length = train(n_episodes=20000, learning_rate=3e-4, clip_grads=True, tau=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[op.name for op in g.get_operations()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "eps_length = [train(n_episodes=2000, learning_rate=3e-4,\n",
    "                    summary='run{}'.format(i), clip_grads=False) for i in range(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "eps_length_clipped = [train(n_episodes=2000, learning_rate=3e-4,\n",
    "                            summary='run{}_clipped'.format(i), clip_grads=True) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(ep_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot episodes length smoothed over 20 runs\n",
    "plt.plot(np.mean(eps_length, axis=0), label='unclipped grads')\n",
    "plt.plot(np.mean(eps_length_clipped, axis=0), label='clipped grads')\n",
    "plt.legend()\n",
    "plt.ylim(0, 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_episode = np.linspace(0, 10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = .01 + (1 - 0.01) * np.exp(-5e-4 * i_episode) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(epsilon)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
