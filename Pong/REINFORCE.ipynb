{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-07 19:32:16,241] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "VALID_ACTIONS = [2, 3]\n",
    "NUM_ACTIONS = len(VALID_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return resize(rgb2gray(img), (84, 84))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, learning_rate):\n",
    "        with tf.variable_scope('policy'):\n",
    "            # Placeholders\n",
    "            self.states = tf.placeholder(name='states', shape=(None, 80, 80), dtype=tf.float32)\n",
    "            self.returns = tf.placeholder(name='returns', shape=(None), dtype=tf.float32)\n",
    "            self.actions = tf.placeholder(name='chosen_action', shape=(None), dtype=tf.int32)\n",
    "\n",
    "            # Network structure\n",
    "            # Convolutional layers\n",
    "            self.conv = slim.stack(self.states, slim.conv2d, [\n",
    "                    (16, 8, 4),\n",
    "                    (32, 4, 3)\n",
    "                ])\n",
    "\n",
    "            # Fully connected layer\n",
    "            flatten = slim.flatten(self.conv)\n",
    "            self.fc = slim.fully_connected(flatten, 216)\n",
    "            # Final/output layer\n",
    "            self.output = slim.fully_connected(self.fc, NUM_ACTION, activation_fn=tf.nn.softmax)        \n",
    "        \n",
    "        # Optimization process (to increase likelihood of a good action)\n",
    "        batch_size = tf.shape(self.states)[0]\n",
    "        # Select the ids of picked actions\n",
    "        # action_ids = (i_batch * NUM_ACTIONS) + action\n",
    "        action_ids = tf.range(batch_size) * tf.shape(self.output)[1] + self.actions\n",
    "        # Select probability of chosen actions\n",
    "        chosen_actions = tf.gather(tf.reshape(self.output, [-1]), action_ids)\n",
    "        eligibility = tf.log(chosen_actions)\n",
    "        # Change the likelihood of taken action using the return (self.returns)        \n",
    "        self.loss = - tf.reduce_mean(self.returns * eligibility)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        # We should perform gradient ascent in the likelihood of specified action\n",
    "        # which is the same as performing gradient descent on the negative of the loss\n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'policy')\n",
    "        grads_and_vars = opt.compute_gradients(self.loss, local_vars)\n",
    "        self.train_op = opt.apply_gradients(grads_and_vars)        \n",
    "        \n",
    "    def predict(self, sess, states):\n",
    "        return sess.run(self.output, feed_dict={self.states: states})\n",
    "    \n",
    "    def update(self, sess, states, actions, returns):\n",
    "        feed_dict = {self.states: states,\n",
    "                     self.actions: actions,\n",
    "                     self.returns: returns}\n",
    "        sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_updates(learning_rate, batch_size=100):\n",
    "    ''' Test if the weigth updates are giving the desired outputs '''\n",
    "    # Create a new policy\n",
    "    tf.reset_default_graph()\n",
    "    policy = Policy(learning_rate=learning_rate)\n",
    "    # Generate states\n",
    "    state = np.random.random((batch_size, 80, 80))\n",
    "    fake_returns = [(100, 'increase'), (-100, 'decrease')]\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        actions = action * np.ones(batch_size)\n",
    "        for fake_return, expected in fake_returns:\n",
    "            # Reinitialize session because ADAM optimizer builds momentum\n",
    "            with tf.Session() as sess:\n",
    "                tf.global_variables_initializer().run()\n",
    "                # Compare new and old probabilities\n",
    "                old_probs = policy.predict(sess, state)\n",
    "                policy.update(sess, state, actions, [fake_return])\n",
    "                new_probs = policy.predict(sess, state)        \n",
    "                print('Action {} probability should {}:'.format(action, expected), end=' ')\n",
    "                print(np.mean(new_probs - old_probs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0 probability should increase: [ 0.03868949 -0.03868948]\n",
      "Action 0 probability should decrease: [-0.05844676  0.05844676]\n",
      "Action 1 probability should increase: [-0.06640681  0.06640682]\n",
      "Action 1 probability should decrease: [ 0.06081842 -0.06081842]\n"
     ]
    }
   ],
   "source": [
    "test_updates(3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mypolicy = Policy(3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    probs = mypolicy.predict(sess, states[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = preprocess(state)\n",
    "# Use a black image for first state\n",
    "states = []\n",
    "states.append(np.zeros((80, 80)))\n",
    "\n",
    "# Repeat until episode is finished\n",
    "for i_step in itertools.count():\n",
    "#    action_probs = policy.predict(sess, states[-1])    \n",
    "    next_state, reward, done, _ = env.step(2)\n",
    "    next_state = preprocess(next_state)\n",
    "    states.append(next_state - state)\n",
    "    #Update state\n",
    "    if done:\n",
    "        break    \n",
    "    state = next_state\n",
    "    \n",
    "states = np.array(states)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(states[259], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
