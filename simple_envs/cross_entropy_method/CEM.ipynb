{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "# ENV_NAME = 'MountainCar-v0'\n",
    "ENV_NAME = 'LunarLander-v2'\n",
    "DISCOUNT_FACTOR = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    return np.squeeze(e_z / np.sum(e_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_action_probs(state, theta):\n",
    "    # Feed foward\n",
    "    z = np.dot(state, theta)\n",
    "    # Softmax map the values to probabilities\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_ep(env, theta, render=False):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "\n",
    "    while not done:    \n",
    "        if render:\n",
    "            env.render()\n",
    "        action_probs = calculate_action_probs(state, theta)\n",
    "        action = np.random.choice(env.action_space.n, p=action_probs)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "#     return calculate_return(rewards, discount_factor)\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_parameters(size, means, std_devs):\n",
    "    thetas = np.array([np.random.normal(size=size, loc=u, scale=d) for u, d in zip(means, std_devs)])\n",
    "    return thetas.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(env, num_episodes, pop_size=32, top_pct=0.1, noise_steps=0.1):\n",
    "    # env information\n",
    "    num_obs = env.observation_space.shape[0]\n",
    "    num_actions = env.action_space.n\n",
    "    # Selected policies\n",
    "    num_idxs = int(top_pct * pop_size)\n",
    "    # One mean and std_dev for each parameter\n",
    "    means = np.random.uniform(size=num_obs * num_actions)\n",
    "    std_devs = np.random.uniform(size=num_obs * num_actions)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        thetas = create_parameters(pop_size, means, std_devs).reshape(pop_size, num_obs, num_actions)\n",
    "        returns = [run_ep(env, theta) for theta in thetas]\n",
    "\n",
    "        # Order best returns        \n",
    "        top_idxs = np.argsort(returns)[::-1][:num_idxs]\n",
    "        best_thetas = thetas[top_idxs]\n",
    "\n",
    "        # New means and std_devs based on top policies\n",
    "        means = np.mean(best_thetas, axis=0).reshape(-1)\n",
    "        # Add some noise because this method converges too quickly\n",
    "        noise = max(((num_episodes / 2) - i_episode) / num_episodes, 0)        \n",
    "        std_devs = np.std(best_thetas, axis=0).reshape(-1) + noise\n",
    "    \n",
    "        print('Episode: {}'.format(i_episode + 1))\n",
    "        print('Theta mean: {}'.format(np.mean(means)))\n",
    "        print('Reward mean: {}'.format(np.mean(returns)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-01 23:55:31,527] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "if 'CartPole' in ENV_NAME:\n",
    "    env._max_episode_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Theta mean: 0.5384641091612756\n",
      "Reward mean: -239.6347200764219\n",
      "\n",
      "Episode: 2\n",
      "Theta mean: 0.7426287055091624\n",
      "Reward mean: -339.3707995380438\n",
      "\n",
      "Episode: 3\n",
      "Theta mean: 0.8376276856190654\n",
      "Reward mean: -343.65047887393354\n",
      "\n",
      "Episode: 4\n",
      "Theta mean: 0.8537331322308048\n",
      "Reward mean: -291.1159082332284\n",
      "\n",
      "Episode: 5\n",
      "Theta mean: 0.7444581870644468\n",
      "Reward mean: -305.8787272701893\n",
      "\n",
      "Episode: 6\n",
      "Theta mean: 0.6781371029423133\n",
      "Reward mean: -288.743269864029\n",
      "\n",
      "Episode: 7\n",
      "Theta mean: 0.8626742304255437\n",
      "Reward mean: -229.5446793036068\n",
      "\n",
      "Episode: 8\n",
      "Theta mean: 0.6349169191031888\n",
      "Reward mean: -203.20513870680415\n",
      "\n",
      "Episode: 9\n",
      "Theta mean: 0.7019893858813642\n",
      "Reward mean: -133.09763408262575\n",
      "\n",
      "Episode: 10\n",
      "Theta mean: 0.8264337195121649\n",
      "Reward mean: -160.19364408722345\n",
      "\n",
      "Episode: 11\n",
      "Theta mean: 0.9217584070399235\n",
      "Reward mean: -146.28843801626084\n",
      "\n",
      "Episode: 12\n",
      "Theta mean: 1.1367826017251346\n",
      "Reward mean: -208.0786079438294\n",
      "\n",
      "Episode: 13\n",
      "Theta mean: 1.032969653436359\n",
      "Reward mean: -138.53732146862026\n",
      "\n",
      "Episode: 14\n",
      "Theta mean: 1.1729210292322492\n",
      "Reward mean: -225.83092834990958\n",
      "\n",
      "Episode: 15\n",
      "Theta mean: 1.3114481095000081\n",
      "Reward mean: -139.92660351049332\n",
      "\n",
      "Episode: 16\n",
      "Theta mean: 1.0595626675674696\n",
      "Reward mean: -97.67544858995305\n",
      "\n",
      "Episode: 17\n",
      "Theta mean: 1.217541760756827\n",
      "Reward mean: -83.98517255539576\n",
      "\n",
      "Episode: 18\n",
      "Theta mean: 1.1495437052858875\n",
      "Reward mean: -45.703675624770796\n",
      "\n",
      "Episode: 19\n",
      "Theta mean: 1.3193442876106127\n",
      "Reward mean: -83.03516561735965\n",
      "\n",
      "Episode: 20\n",
      "Theta mean: 1.2750690141864363\n",
      "Reward mean: -30.152680020636648\n",
      "\n",
      "Episode: 21\n",
      "Theta mean: 1.2358928947231\n",
      "Reward mean: -31.404821817986033\n",
      "\n",
      "Episode: 22\n",
      "Theta mean: 1.033313947820945\n",
      "Reward mean: -1.4379500518721748\n",
      "\n",
      "Episode: 23\n",
      "Theta mean: 0.8881524112275079\n",
      "Reward mean: -43.97358020738297\n",
      "\n",
      "Episode: 24\n",
      "Theta mean: 0.8591908381583396\n",
      "Reward mean: -31.31881867736231\n",
      "\n",
      "Episode: 25\n",
      "Theta mean: 0.9326636882008184\n",
      "Reward mean: 10.258379301197982\n",
      "\n",
      "Episode: 26\n",
      "Theta mean: 1.2206062415005134\n",
      "Reward mean: -11.706172130655975\n",
      "\n",
      "Episode: 27\n",
      "Theta mean: 1.267219480397955\n",
      "Reward mean: 28.66946726276404\n",
      "\n",
      "Episode: 28\n",
      "Theta mean: 0.9504460158038324\n",
      "Reward mean: 38.75691513747062\n",
      "\n",
      "Episode: 29\n",
      "Theta mean: 0.8038057480904162\n",
      "Reward mean: 70.92362331455573\n",
      "\n",
      "Episode: 30\n",
      "Theta mean: 0.7532966339255026\n",
      "Reward mean: 72.12767847513487\n",
      "\n",
      "Episode: 31\n",
      "Theta mean: 0.9327299825653179\n",
      "Reward mean: 59.850680768340965\n",
      "\n",
      "Episode: 32\n",
      "Theta mean: 0.8349854531176812\n",
      "Reward mean: 133.70199900692097\n",
      "\n",
      "Episode: 33\n",
      "Theta mean: 0.9068143219133519\n",
      "Reward mean: 104.3924328479459\n",
      "\n",
      "Episode: 34\n",
      "Theta mean: 0.8999058691937212\n",
      "Reward mean: 81.37615017743042\n",
      "\n",
      "Episode: 35\n",
      "Theta mean: 0.8859598717225062\n",
      "Reward mean: 57.11853786482878\n",
      "\n",
      "Episode: 36\n",
      "Theta mean: 0.7675095646236172\n",
      "Reward mean: 101.57037155619619\n",
      "\n",
      "Episode: 37\n",
      "Theta mean: 0.5091668748997429\n",
      "Reward mean: 120.58644203821021\n",
      "\n",
      "Episode: 38\n",
      "Theta mean: 0.4764955809783082\n",
      "Reward mean: 102.40247012840645\n",
      "\n",
      "Episode: 39\n",
      "Theta mean: 0.41199251603126796\n",
      "Reward mean: 82.47417551656002\n",
      "\n",
      "Episode: 40\n",
      "Theta mean: 0.43064940159409604\n",
      "Reward mean: 88.06946273104657\n",
      "\n",
      "Episode: 41\n",
      "Theta mean: 0.4003947937037078\n",
      "Reward mean: 151.5388177804294\n",
      "\n",
      "Episode: 42\n",
      "Theta mean: 0.29755137808923615\n",
      "Reward mean: 156.12538938830002\n",
      "\n",
      "Episode: 43\n",
      "Theta mean: 0.23177278088910014\n",
      "Reward mean: 128.85014388925987\n",
      "\n",
      "Episode: 44\n",
      "Theta mean: 0.24286564615754735\n",
      "Reward mean: 124.73736344648916\n",
      "\n",
      "Episode: 45\n",
      "Theta mean: 0.24644115286724022\n",
      "Reward mean: 134.59731597591005\n",
      "\n",
      "Episode: 46\n",
      "Theta mean: 0.28739329564635757\n",
      "Reward mean: 151.1261869222708\n",
      "\n",
      "Episode: 47\n",
      "Theta mean: 0.30844853679407774\n",
      "Reward mean: 105.06655926170669\n",
      "\n",
      "Episode: 48\n",
      "Theta mean: 0.32171673254847083\n",
      "Reward mean: 169.87050298812588\n",
      "\n",
      "Episode: 49\n",
      "Theta mean: 0.3344023134216844\n",
      "Reward mean: 136.5674690483446\n",
      "\n",
      "Episode: 50\n",
      "Theta mean: 0.32115431536411004\n",
      "Reward mean: 179.9080174951436\n",
      "\n",
      "Episode: 51\n",
      "Theta mean: 0.30272092136483325\n",
      "Reward mean: 142.33788301674994\n",
      "\n",
      "Episode: 52\n",
      "Theta mean: 0.31727868749825705\n",
      "Reward mean: 153.6338257884542\n",
      "\n",
      "Episode: 53\n",
      "Theta mean: 0.3392588000442332\n",
      "Reward mean: 155.6755833012998\n",
      "\n",
      "Episode: 54\n",
      "Theta mean: 0.33461411864629215\n",
      "Reward mean: 166.75873726057037\n",
      "\n",
      "Episode: 55\n",
      "Theta mean: 0.34688431397271247\n",
      "Reward mean: 131.06154352264468\n",
      "\n",
      "Episode: 56\n",
      "Theta mean: 0.35287825685924495\n",
      "Reward mean: 174.67961025976206\n",
      "\n",
      "Episode: 57\n",
      "Theta mean: 0.3551486834046505\n",
      "Reward mean: 144.1135639998377\n",
      "\n",
      "Episode: 58\n",
      "Theta mean: 0.36011275796624875\n",
      "Reward mean: 119.84891534929022\n",
      "\n",
      "Episode: 59\n",
      "Theta mean: 0.35992040973249745\n",
      "Reward mean: 149.8851501625903\n",
      "\n",
      "Episode: 60\n",
      "Theta mean: 0.3611780287806639\n",
      "Reward mean: 138.39641633362706\n",
      "\n",
      "Episode: 61\n",
      "Theta mean: 0.36074989418048464\n",
      "Reward mean: 149.03690427301075\n",
      "\n",
      "Episode: 62\n",
      "Theta mean: 0.36074400186405337\n",
      "Reward mean: 138.205104112501\n",
      "\n",
      "Episode: 63\n",
      "Theta mean: 0.36095211273276256\n",
      "Reward mean: 158.66120236169047\n",
      "\n",
      "Episode: 64\n",
      "Theta mean: 0.36039006823842845\n",
      "Reward mean: 165.18315681208884\n",
      "\n",
      "Episode: 65\n",
      "Theta mean: 0.36044066226606797\n",
      "Reward mean: 139.13998010663784\n",
      "\n",
      "Episode: 66\n",
      "Theta mean: 0.36050692146584873\n",
      "Reward mean: 131.91073288746549\n",
      "\n",
      "Episode: 67\n",
      "Theta mean: 0.3605395777466531\n",
      "Reward mean: 147.10217866587692\n",
      "\n",
      "Episode: 68\n",
      "Theta mean: 0.36052000778410564\n",
      "Reward mean: 133.19655800673218\n",
      "\n",
      "Episode: 69\n",
      "Theta mean: 0.36051644123385584\n",
      "Reward mean: 136.55564333770374\n",
      "\n",
      "Episode: 70\n",
      "Theta mean: 0.3604977115555247\n",
      "Reward mean: 111.90840373118814\n",
      "\n",
      "Episode: 71\n",
      "Theta mean: 0.3605130406101448\n",
      "Reward mean: 148.38411521527286\n",
      "\n",
      "Episode: 72\n",
      "Theta mean: 0.3605130277165082\n",
      "Reward mean: 133.652955877912\n",
      "\n",
      "Episode: 73\n",
      "Theta mean: 0.36053814325697636\n",
      "Reward mean: 154.04231854740615\n",
      "\n",
      "Episode: 74\n",
      "Theta mean: 0.3605413829887699\n",
      "Reward mean: 147.07599896689746\n",
      "\n",
      "Episode: 75\n",
      "Theta mean: 0.36052921088321344\n",
      "Reward mean: 174.70022658318945\n",
      "\n",
      "Episode: 76\n",
      "Theta mean: 0.36051476893876994\n",
      "Reward mean: 159.30810741558187\n",
      "\n",
      "Episode: 77\n",
      "Theta mean: 0.3605156045443729\n",
      "Reward mean: 147.98894122647687\n",
      "\n",
      "Episode: 78\n",
      "Theta mean: 0.3605405363317604\n",
      "Reward mean: 128.01563212393302\n",
      "\n",
      "Episode: 79\n",
      "Theta mean: 0.3605272513061303\n",
      "Reward mean: 165.5742063068393\n",
      "\n",
      "Episode: 80\n",
      "Theta mean: 0.3605465429213839\n",
      "Reward mean: 173.62226272637463\n",
      "\n",
      "Episode: 81\n",
      "Theta mean: 0.3605373066460317\n",
      "Reward mean: 153.61252000669685\n",
      "\n",
      "Episode: 82\n",
      "Theta mean: 0.36053947255898916\n",
      "Reward mean: 163.70017193422257\n",
      "\n",
      "Episode: 83\n",
      "Theta mean: 0.3605391266037772\n",
      "Reward mean: 129.92096801155077\n",
      "\n",
      "Episode: 84\n",
      "Theta mean: 0.3605394605990766\n",
      "Reward mean: 134.75313250981662\n",
      "\n",
      "Episode: 85\n",
      "Theta mean: 0.3605395344085941\n",
      "Reward mean: 135.20108107208358\n",
      "\n",
      "Episode: 86\n",
      "Theta mean: 0.36053953419327034\n",
      "Reward mean: 141.7177754898314\n",
      "\n",
      "Episode: 87\n",
      "Theta mean: 0.36053945418885075\n",
      "Reward mean: 155.89685128096284\n",
      "\n",
      "Episode: 88\n",
      "Theta mean: 0.36053944942894267\n",
      "Reward mean: 127.36835699044029\n",
      "\n",
      "Episode: 89\n",
      "Theta mean: 0.3605394205923907\n",
      "Reward mean: 112.35620673980449\n",
      "\n",
      "Episode: 90\n",
      "Theta mean: 0.3605394143992139\n",
      "Reward mean: 120.90548380372884\n",
      "\n",
      "Episode: 91\n",
      "Theta mean: 0.3605394120711208\n",
      "Reward mean: 138.08058127903558\n",
      "\n",
      "Episode: 92\n",
      "Theta mean: 0.3605394098363016\n",
      "Reward mean: 125.16296018731497\n",
      "\n",
      "Episode: 93\n",
      "Theta mean: 0.36053941006476237\n",
      "Reward mean: 130.44961117559777\n",
      "\n",
      "Episode: 94\n",
      "Theta mean: 0.3605394105352039\n",
      "Reward mean: 145.15293992855794\n",
      "\n",
      "Episode: 95\n",
      "Theta mean: 0.3605394106113606\n",
      "Reward mean: 150.8524818187139\n",
      "\n",
      "Episode: 96\n",
      "Theta mean: 0.36053941110628457\n",
      "Reward mean: 163.85000819475277\n",
      "\n",
      "Episode: 97\n",
      "Theta mean: 0.3605394111831091\n",
      "Reward mean: 124.91226481765995\n",
      "\n",
      "Episode: 98\n",
      "Theta mean: 0.36053941151685037\n",
      "Reward mean: 135.76140853641326\n",
      "\n",
      "Episode: 99\n",
      "Theta mean: 0.3605394118423243\n",
      "Reward mean: 151.77211603685777\n",
      "\n",
      "Episode: 100\n",
      "Theta mean: 0.3605394116591875\n",
      "Reward mean: 135.65544891804308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimize(env, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
