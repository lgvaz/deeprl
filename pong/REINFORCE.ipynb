{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-10 15:55:10,362] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "VALID_ACTIONS = [2, 3]\n",
    "NUM_ACTIONS = len(VALID_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return resize(rgb2gray(img), (84, 84))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reward_return(rewards, discount_factor=0.99):\n",
    "    ''' Calculate the return from step t '''\n",
    "    G = []\n",
    "    G_sum = 0\n",
    "    # Start calculating from the last reward\n",
    "    for t, reward in enumerate(reversed(rewards)):\n",
    "        G_sum = G_sum * discount_factor + reward\n",
    "        G.append(G_sum)\n",
    "    # Revert list again\n",
    "    G.reverse()\n",
    "    # Normalize returns\n",
    "    G = (G - np.mean(G)) / np.std(G)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shared_network(inputs, reuse):\n",
    "    '''\n",
    "    Builds 2 convolutional layers and a fully connected layer at the end\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input image for the network \n",
    "    \n",
    "    Returns:\n",
    "        The last convolutional layer and the fully connect layer\n",
    "    '''\n",
    "    with tf.variable_scope('shared', reuse=reuse):\n",
    "        # Convolutional layers\n",
    "        conv = slim.stack(inputs, slim.conv2d, [\n",
    "                (16, 8, 4),\n",
    "                (32, 4, 3)\n",
    "            ])\n",
    "\n",
    "        # Fully connected layer\n",
    "        flatten = slim.flatten(conv)\n",
    "        fc = slim.fully_connected(flatten, 216)\n",
    "        \n",
    "        return conv, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, learning_rate, reuse=False):\n",
    "        # Placeholders\n",
    "        self.states = tf.placeholder(name='states',\n",
    "                                     shape=(None, 80, 80),\n",
    "                                     dtype=tf.float32)\n",
    "        self.returns = tf.placeholder(name='returns',\n",
    "                                      shape=(None),\n",
    "                                      dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(name='chosen_action',\n",
    "                                      shape=(None),\n",
    "                                      dtype=tf.int32)\n",
    "\n",
    "        # Create shared network\n",
    "        self.conv, self.fc = shared_network(self.states, reuse)\n",
    "        with tf.variable_scope('policy'):\n",
    "            # Final/output layer\n",
    "            self.output = slim.fully_connected(self.fc,\n",
    "                                               NUM_ACTIONS,\n",
    "                                               activation_fn=tf.nn.softmax)        \n",
    "        \n",
    "        # Optimization process (to increase likelihood of a good action)\n",
    "        batch_size = tf.shape(self.states)[0]\n",
    "        # Select the ids of picked actions\n",
    "        # action_ids = (i_batch * NUM_ACTIONS) + action\n",
    "        action_ids = tf.range(batch_size) * tf.shape(self.output)[1] + self.actions\n",
    "        # Select probability of chosen actions\n",
    "        chosen_actions = tf.gather(tf.reshape(self.output, [-1]), action_ids)\n",
    "        eligibility = tf.log(chosen_actions)\n",
    "        # Change the likelihood of taken action using the return (self.returns)        \n",
    "        self.loss = - tf.reduce_mean(self.returns * eligibility)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        # We should perform gradient ascent in the likelihood of specified action\n",
    "        # which is the same as performing gradient descent on the negative of the loss\n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'policy') \\\n",
    "                     + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'shared') \n",
    "        grads_and_vars = opt.compute_gradients(self.loss, local_vars)\n",
    "        self.global_step = slim.get_or_create_global_step()\n",
    "        self.train_op = opt.apply_gradients(grads_and_vars, self.global_step)        \n",
    "        \n",
    "        # Add summaries\n",
    "        tf.summary.histogram('convolution', self.conv)\n",
    "        tf.summary.histogram('last_hidden', self.fc)\n",
    "        tf.summary.histogram('action_probs', self.output)\n",
    "        tf.summary.scalar('policy_loss', self.loss)\n",
    "        \n",
    "    def predict(self, sess, states):\n",
    "        return sess.run(self.output, feed_dict={self.states: states})\n",
    "    \n",
    "    def update(self, sess, states, actions, returns):\n",
    "        feed_dict = {self.states: states,\n",
    "                     self.actions: actions,\n",
    "                     self.returns: returns}\n",
    "        sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ValueNet:\n",
    "    def __init__(self, learning_rate, reuse=False):\n",
    "        # Placeholders\n",
    "        self.states = tf.placeholder(name='states',\n",
    "                                     shape=(None, 80, 80),\n",
    "                                     dtype=tf.float32)\n",
    "        # TD targets\n",
    "        self.targets = tf.placeholder(name='targets',\n",
    "                                      shape=(None),\n",
    "                                      dtype=tf.float32)\n",
    "        \n",
    "        # Get or create shared network\n",
    "        self.conv, self.fc = shared_network(self.states, reuse)\n",
    "        # Final/output layer\n",
    "        with tf.variable_scope('value_net'):\n",
    "            self.output = slim.fully_connected(inputs=self.fc,\n",
    "                                               num_outputs=1,\n",
    "                                               activation_fn=None)\n",
    "        \n",
    "        # Loss (mean squared error)\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.targets, self.output))\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'value_net') \\\n",
    "                   + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'shared') \n",
    "        grads_and_vars = opt.compute_gradients(self.loss, local_vars)\n",
    "        self.global_step = slim.get_or_create_global_step()\n",
    "        self.train_op = opt.apply_gradients(grads_and_vars)#, self.global_step)        \n",
    "        \n",
    "        # Add summaries\n",
    "        tf.summary.histogram('state_values', self.output)\n",
    "        tf.summary.scalar('average_state_value', tf.reduce_mean(self.output))\n",
    "        tf.summary.scalar('max_state_value', tf.reduce_max(self.output))\n",
    "        tf.summary.scalar('baseline_loss', self.loss)\n",
    "        \n",
    "    def predict(self, sess, states):\n",
    "        return sess.run(self.output, feed_dict={self.states: states})\n",
    "    \n",
    "    def update(self, sess, states, targets):\n",
    "        feed_dict = {self.states: states,\n",
    "                     self.targets: targets}\n",
    "        sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_updates(learning_rate, batch_size=100):\n",
    "    ''' Test if the weigth updates are giving the desired outputs '''\n",
    "    # Create a new policy\n",
    "    tf.reset_default_graph()\n",
    "    policy = Policy(learning_rate=learning_rate, reuse=False)\n",
    "    baseline = ValueNet(learning_rate=learning_rate, reuse=True)\n",
    "    # Generate states\n",
    "    state = np.random.random((batch_size, 80, 80))\n",
    "    fake_returns = [(100, 'increase'), (-100, 'decrease')]\n",
    "    print('Testing policy updates...')\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        actions = action * np.ones(batch_size)\n",
    "        for fake_return, expected in fake_returns:\n",
    "            # Reinitialize session because ADAM optimizer builds momentum\n",
    "            with tf.Session() as sess:\n",
    "                tf.global_variables_initializer().run()\n",
    "                # Compare new and old probabilities\n",
    "                old_probs = policy.predict(sess, state)\n",
    "                policy.update(sess, state, actions, [fake_return])\n",
    "                new_probs = policy.predict(sess, state)        \n",
    "                print('Action {} probability should {}:'.format(action, expected), end=' ')\n",
    "                print(np.mean(new_probs - old_probs, axis=0))\n",
    "    \n",
    "    print('\\nTesting baseline updates...')\n",
    "    targets = 100 * np.ones(batch_size)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        old_value = baseline.predict(sess, state)\n",
    "        baseline.update(sess, state, targets)\n",
    "        new_value = baseline.predict(sess, state)\n",
    "        value_change = np.mean(new_value - old_value)\n",
    "        print('Value of states should increase: {}'.format(value_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary_writer_op(sess, logdir, policy_net, value_net):\n",
    "    '''\n",
    "    Merge all summaries and returns an function to\n",
    "    write the summary\n",
    "    '''\n",
    "    # Check if path already exists\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    \n",
    "    # Create placeholders to track some statistics\n",
    "    episode_reward = tf.placeholder(name='episode_reward',\n",
    "                                         shape=(),\n",
    "                                         dtype=tf.float32)\n",
    "    episode_length = tf.placeholder(name='episode_length',\n",
    "                                         shape=(),\n",
    "                                         dtype=tf.float32)\n",
    "    # Create some summaries\n",
    "    tf.summary.scalar('reward', episode_reward)\n",
    "    tf.summary.scalar('episode_length', episode_length)\n",
    "    \n",
    "    # Merge all summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "    global_step = slim.get_or_create_global_step()\n",
    "    \n",
    "    def summary_writer(states, actions, returns, targets, reward, length):\n",
    "        feed_dict = {\n",
    "            policy_net.states: states,\n",
    "            policy_net.actions: actions,\n",
    "            policy_net.returns: returns,\n",
    "            value_net.states: states,\n",
    "            value_net.targets: targets,\n",
    "            episode_reward: reward,\n",
    "            episode_length: length\n",
    "        }        \n",
    "        summary, step = sess.run([merged, global_step],\n",
    "                                 feed_dict=feed_dict)\n",
    "        # Write summary\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "    return summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing policy updates...\n",
      "Action 0 probability should increase: [ 0.09512234 -0.09512234]\n",
      "Action 0 probability should decrease: [-0.09389946  0.09389948]\n",
      "Action 1 probability should increase: [-0.07670062  0.07670062]\n",
      "Action 1 probability should decrease: [ 0.05909234 -0.05909234]\n",
      "\n",
      "Testing baseline updates...\n",
      "Value of states should increase: 0.25253012776374817\n"
     ]
    }
   ],
   "source": [
    "test_updates(3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_episodes = 500000\n",
    "logdir = 'summaries/run1'\n",
    "savedir = 'checkpoints'\n",
    "learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest checkpoint...\n",
      "Started training...\n",
      "Episode 5670/500000 | Reward: -13.0"
     ]
    }
   ],
   "source": [
    "# Create networks\n",
    "tf.reset_default_graph()\n",
    "policy = Policy(learning_rate=learning_rate, \n",
    "                reuse=False)\n",
    "baseline = ValueNet(learning_rate=learning_rate,\n",
    "                    reuse=True)\n",
    "\n",
    "# Create checkpoint directory\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "save_path = os.path.join(savedir, 'graph.ckpt')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Create tensorflow saver\n",
    "    saver = tf.train.Saver()\n",
    "    # Verify if a checkpoint already exists\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(savedir)\n",
    "    if latest_checkpoint is not None:\n",
    "        print('Loading latest checkpoint...')\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    else:\n",
    "        tf.global_variables_initializer().run()\n",
    "    # Create write summary op\n",
    "    write_summary = summary_writer_op(sess=sess,\n",
    "                                      logdir=logdir,\n",
    "                                      policy_net=policy,\n",
    "                                      value_net=baseline)\n",
    "    \n",
    "    print('Started training...')\n",
    "    for i_episode in range(num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        state = preprocess(state)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        # Use a black image for first state\n",
    "        states.append(np.zeros((80, 80)))\n",
    "\n",
    "        # Repeat until episode is finished\n",
    "        for i_step in itertools.count():\n",
    "#            env.render()\n",
    "            # Choose an action\n",
    "            action_probs = np.squeeze(policy.predict(sess, states[-1][np.newaxis, ...]))\n",
    "            action = np.random.choice(np.arange(NUM_ACTIONS), p=action_probs)\n",
    "            valid_action = VALID_ACTIONS[action]\n",
    "            # Do the action\n",
    "            next_state, reward, done, _ = env.step(valid_action)\n",
    "            next_state = preprocess(next_state)\n",
    "            # Store experience\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            #Update state\n",
    "            if done:\n",
    "                break    \n",
    "            states.append(next_state - state)\n",
    "            state = next_state\n",
    "\n",
    "        # Calculate episode returns\n",
    "        returns = reward_return(rewards)\n",
    "        states = np.array(states) \n",
    "        actions = np.array(actions)\n",
    "        \n",
    "        # Calculate baseline values\n",
    "        states_value = baseline.predict(sess, states)\n",
    "        base_returns = returns - np.squeeze(states_value)\n",
    "        \n",
    "        # Update baseline weights\n",
    "        baseline.update(sess, states, returns)\n",
    "        # Update policy weights\n",
    "        policy.update(sess, states, actions, base_returns)       \n",
    "        \n",
    "        # Write summaries\n",
    "        write_summary(states=states,\n",
    "                      actions=actions,\n",
    "                      returns=base_returns,\n",
    "                      targets=returns,\n",
    "                      reward=np.sum(rewards),\n",
    "                      length=i_step)\n",
    "        \n",
    "        # Save graph\n",
    "        if i_episode % 100 == 0:\n",
    "            saver.save(sess, save_path)\n",
    "        \n",
    "        # Print information\n",
    "        print('\\rEpisode {}/{}'.format(i_episode, num_episodes), end=' | ')\n",
    "        print('Reward: {}'.format(np.sum(rewards)), end='')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
