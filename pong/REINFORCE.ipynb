{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-08 19:16:40,359] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "VALID_ACTIONS = [2, 3]\n",
    "NUM_ACTIONS = len(VALID_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    return resize(rgb2gray(img), (84, 84))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shared_network(inputs, reuse):\n",
    "    '''\n",
    "    Builds 2 convolutional layers and a fully connected layer at the end\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input image for the network \n",
    "    \n",
    "    Returns:\n",
    "        The last convolutional layer and the fully connect layer\n",
    "    '''\n",
    "    with tf.variable_scope('shared', reuse=reuse):\n",
    "        # Convolutional layers\n",
    "        conv = slim.stack(inputs, slim.conv2d, [\n",
    "                (16, 8, 4),\n",
    "                (32, 4, 3)\n",
    "            ])\n",
    "\n",
    "        # Fully connected layer\n",
    "        flatten = slim.flatten(conv)\n",
    "        fc = slim.fully_connected(flatten, 216)\n",
    "        \n",
    "        return conv, fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, learning_rate, reuse=False):\n",
    "        # Placeholders\n",
    "        self.states = tf.placeholder(name='states',\n",
    "                                     shape=(None, 80, 80),\n",
    "                                     dtype=tf.float32)\n",
    "        self.returns = tf.placeholder(name='returns',\n",
    "                                      shape=(None),\n",
    "                                      dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(name='chosen_action',\n",
    "                                      shape=(None),\n",
    "                                      dtype=tf.int32)\n",
    "\n",
    "        # Create shared network\n",
    "        self.conv, self.fc = shared_network(self.states, reuse)\n",
    "        with tf.variable_scope('policy'):\n",
    "            # Final/output layer\n",
    "            self.output = slim.fully_connected(self.fc,\n",
    "                                               NUM_ACTIONS,\n",
    "                                               activation_fn=tf.nn.softmax)        \n",
    "        \n",
    "        # Optimization process (to increase likelihood of a good action)\n",
    "        batch_size = tf.shape(self.states)[0]\n",
    "        # Select the ids of picked actions\n",
    "        # action_ids = (i_batch * NUM_ACTIONS) + action\n",
    "        action_ids = tf.range(batch_size) * tf.shape(self.output)[1] + self.actions\n",
    "        # Select probability of chosen actions\n",
    "        chosen_actions = tf.gather(tf.reshape(self.output, [-1]), action_ids)\n",
    "        eligibility = tf.log(chosen_actions)\n",
    "        # Change the likelihood of taken action using the return (self.returns)        \n",
    "        self.loss = - tf.reduce_mean(self.returns * eligibility)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        # We should perform gradient ascent in the likelihood of specified action\n",
    "        # which is the same as performing gradient descent on the negative of the loss\n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'policy') \\\n",
    "                     + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'shared') \n",
    "        grads_and_vars = opt.compute_gradients(self.loss, local_vars)\n",
    "        self.global_step = slim.get_or_create_global_step()\n",
    "        self.train_op = opt.apply_gradients(grads_and_vars, self.global_step)        \n",
    "        \n",
    "        # Add summaries\n",
    "        tf.summary.histogram('convolution', self.conv)\n",
    "        tf.summary.histogram('last_hidden', self.fc)\n",
    "        tf.summary.histogram('action_probs', self.output)\n",
    "        tf.summary.scalar('policy_loss', self.loss)\n",
    "        \n",
    "    def predict(self, sess, states):\n",
    "        return sess.run(self.output, feed_dict={self.states: states})\n",
    "    \n",
    "    def update(self, sess, states, actions, returns):\n",
    "        feed_dict = {self.states: states,\n",
    "                     self.actions: actions,\n",
    "                     self.returns: returns}\n",
    "        sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ValueNet:\n",
    "    def __init__(self, learning_rate, reuse=False):\n",
    "        # Placeholders\n",
    "        self.states = tf.placeholder(name='states',\n",
    "                                     shape=(None, 80, 80),\n",
    "                                     dtype=tf.float32)\n",
    "        # TD targets\n",
    "        self.targets = tf.placeholder(name='targets',\n",
    "                                      shape=(None),\n",
    "                                      dtype=tf.float32)\n",
    "        \n",
    "        # Get or create shared network\n",
    "        self.conv, self.fc = shared_network(self.states, reuse)\n",
    "        # Final/output layer\n",
    "        with tf.variable_scope('value_net'):\n",
    "            self.output = slim.fully_connected(inputs=self.fc,\n",
    "                                               num_outputs=1,\n",
    "                                               activation_fn=None)\n",
    "        \n",
    "        # Loss (mean squared error)\n",
    "        self.loss = tf.reduce_mean(tf.squared_difference(self.targets, self.output))\n",
    "        opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'value_net') \\\n",
    "                   + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'shared') \n",
    "        grads_and_vars = opt.compute_gradients(self.loss, local_vars)\n",
    "        self.global_step = slim.get_or_create_global_step()\n",
    "        self.train_op = opt.apply_gradients(grads_and_vars, self.global_step)        \n",
    "        \n",
    "        # Add summaries\n",
    "        tf.summary.histogram('state_values', self.output)\n",
    "        tf.summary.scalar('average_value', tf.reduce_mean(self.output))\n",
    "        tf.summary.scalar('max_value', tf.reduce_max(self.output))\n",
    "        tf.summary.scalar('baseline_loss', self.loss)\n",
    "        \n",
    "    def predict(self, sess, states):\n",
    "        return sess.run(self.output, feed_dict={self.states: states})\n",
    "    \n",
    "    def update(self, sess, states, targets):\n",
    "        feed_dict = {self.states: states,\n",
    "                     self.targets: targets}\n",
    "        sess.run(self.train_op, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_updates(learning_rate, batch_size=100):\n",
    "    ''' Test if the weigth updates are giving the desired outputs '''\n",
    "    # Create a new policy\n",
    "    tf.reset_default_graph()\n",
    "    policy = Policy(learning_rate=learning_rate, reuse=False)\n",
    "    baseline = ValueNet(learning_rate=learning_rate, reuse=True)\n",
    "    # Generate states\n",
    "    state = np.random.random((batch_size, 80, 80))\n",
    "    fake_returns = [(100, 'increase'), (-100, 'decrease')]\n",
    "    print('Testing policy updates...')\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        actions = action * np.ones(batch_size)\n",
    "        for fake_return, expected in fake_returns:\n",
    "            # Reinitialize session because ADAM optimizer builds momentum\n",
    "            with tf.Session() as sess:\n",
    "                tf.global_variables_initializer().run()\n",
    "                # Compare new and old probabilities\n",
    "                old_probs = policy.predict(sess, state)\n",
    "                policy.update(sess, state, actions, [fake_return])\n",
    "                new_probs = policy.predict(sess, state)        \n",
    "                print('Action {} probability should {}:'.format(action, expected), end=' ')\n",
    "                print(np.mean(new_probs - old_probs, axis=0))\n",
    "    \n",
    "    print('\\nTesting baseline updates...')\n",
    "    targets = 100 * np.ones(batch_size)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        old_value = baseline.predict(sess, state)\n",
    "        baseline.update(sess, state, targets)\n",
    "        new_value = baseline.predict(sess, state)\n",
    "        value_change = np.mean(new_value - old_value)\n",
    "        print('Value of states should increase: {}'.format(value_change))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing policy updates...\n",
      "Action 0 probability should increase: [ 0.04969514 -0.04969513]\n",
      "Action 0 probability should decrease: [-0.06352526  0.06352527]\n",
      "Action 1 probability should increase: [-0.03871953  0.03871953]\n",
      "Action 1 probability should decrease: [ 0.02817643 -0.02817643]\n",
      "\n",
      "Testing baseline updates...\n",
      "Value of states should increase: 0.1389562040567398\n"
     ]
    }
   ],
   "source": [
    "test_updates(3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary_writer_op(sess, logdir, policy_net, value_net):\n",
    "    '''\n",
    "    Merge all summaries and returns an function to\n",
    "    write the summary\n",
    "    '''\n",
    "    # Check if path already exists\n",
    "    if not os.path.exists(logdir):\n",
    "        os.makedirs(logdir)\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "    \n",
    "    # Create placeholders to track some statistics\n",
    "    episode_reward = tf.placeholder(name='episode_reward',\n",
    "                                         shape=(),\n",
    "                                         dtype=tf.float32)\n",
    "    episode_length = tf.placeholder(name='episode_length',\n",
    "                                         shape=(),\n",
    "                                         dtype=tf.float32)\n",
    "    # Create some summaries\n",
    "    tf.summary.scalar('reward', episode_reward)\n",
    "    tf.summary.scalar('episode_length', episode_length)\n",
    "    \n",
    "    # Merge all summaries\n",
    "    merged = tf.summary.merge_all()    \n",
    "    global_step = slim.get_or_create_global_step()\n",
    "    \n",
    "    def summary_writer(sess, states, actions, returns, targets, reward, length):\n",
    "        feed_dict = {\n",
    "            policy_net.states: states,\n",
    "            policy_net.actions: actions,\n",
    "            policy_net.returns: returns,\n",
    "            value_net.states: states,\n",
    "            value_net.targets: targets,\n",
    "            episode_reward: reward,\n",
    "            episode_length: length\n",
    "        }        \n",
    "        summary, step = sess.run([merged, global_step],\n",
    "                                 feed_dict=feed_dict)\n",
    "        # Write summary\n",
    "        writer.add_summary(summary, step)\n",
    "        \n",
    "    return summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = preprocess(state)\n",
    "# Use a black image for first state\n",
    "states = []\n",
    "states.append(np.zeros((80, 80)))\n",
    "\n",
    "# Repeat until episode is finished\n",
    "for i_step in itertools.count():\n",
    "#    action_probs = policy.predict(sess, states[-1])    \n",
    "    next_state, reward, done, _ = env.step(2)\n",
    "    next_state = preprocess(next_state)\n",
    "    states.append(next_state - state)\n",
    "    #Update state\n",
    "    if done:\n",
    "        break    \n",
    "    state = next_state\n",
    "    \n",
    "states = np.array(states)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(states[259], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
