{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.layers import xavier_initializer_conv2d\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-03 21:13:16,923] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "NUM_STACKED_FRAMES = 4\n",
    "NUM_ACTIONS = 2\n",
    "VALID_ACTIONS = [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    ''' Converts image to grayscale and resizes to 84x84 '''\n",
    "    # Maybe try preprocess on TF? Faster on GPU?\n",
    "    return resize(rgb2gray(img), (84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    ''' \n",
    "    Creates a buffer that stores experience (replays)\n",
    "    '''\n",
    "    def __init__(self, max_replays):\n",
    "        self.max_replays = max_replays\n",
    "        # Allocate memory\n",
    "        self.imgs = np.empty((max_replays, 84, 84), dtype=np.float32)\n",
    "        self.actions = np.empty(max_replays, dtype=np.int32)\n",
    "        self.rewards = np.empty(max_replays, dtype=np.float32)\n",
    "        self.done = np.empty(max_replays, dtype=np.bool)\n",
    "        # Create \"pointers\"\n",
    "        self.current = 0 \n",
    "        self.bottom = 0\n",
    "        self.size = 0\n",
    "        \n",
    "    def add_replay(self, img, action, reward, done):\n",
    "        '''  Add experience to memory ''' \n",
    "        self.imgs[self.current] = img\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.done[self.current] = done\n",
    "        # Update memory actual size\n",
    "        if self.size == self.max_replays:\n",
    "            self.bottom = (self.bottom + 1) % self.max_replays\n",
    "        else:\n",
    "            self.size += 1\n",
    "        # Update current memory \"pointer\"\n",
    "        self.current = (self.current + 1) % self.max_replays\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ''' \n",
    "        Return an array containing NUM_STACKED_FRAMES sequential imgs.\n",
    "        '''\n",
    "        batch_pre_states = np.empty((batch_size, NUM_STACKED_FRAMES, 84, 84), dtype=np.float32)\n",
    "        batch_pos_states = np.empty((batch_size, NUM_STACKED_FRAMES, 84, 84), dtype=np.float32)\n",
    "        batch_actions = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_rewards = np.empty(batch_size, dtype=np.float32)\n",
    "        batch_done = np.empty(batch_size, dtype=np.bool)\n",
    "        \n",
    "        i_batch = 0\n",
    "        while i_batch < batch_size:\n",
    "            # Get a random idx to construct a single transition\n",
    "            start_idx = np.random.randint(self.bottom, self.bottom + self.size - NUM_STACKED_FRAMES - 1)\n",
    "            pre_states_idx = np.arange(start_idx, start_idx + NUM_STACKED_FRAMES)\n",
    "            # Only the last frame of pre states can be a finished episode\n",
    "            if np.any(self.done.take(pre_states_idx[:-1], axis=0, mode='wrap')):\n",
    "                continue\n",
    "            pos_states_idx = np.arange(start_idx + 1, start_idx + 1 + NUM_STACKED_FRAMES)\n",
    "\n",
    "            # Save transitions\n",
    "            pre_end_idx = pre_states_idx[-1]\n",
    "            batch_pre_states[i_batch] = self.imgs.take(pre_states_idx, axis=0, mode='wrap')\n",
    "            batch_pos_states[i_batch] = self.imgs.take(pos_states_idx, axis=0, mode='wrap')\n",
    "            batch_actions[i_batch] = self.actions.take(pre_end_idx, axis=0, mode='wrap')\n",
    "            batch_rewards[i_batch] = self.rewards.take(pre_end_idx, axis=0, mode='wrap')\n",
    "            batch_done[i_batch] = self.done.take(pre_end_idx, axis=0, mode='wrap')\n",
    "            i_batch += 1\n",
    "        \n",
    "        # Roll axis to be of shape: (batch_size, 84, 84, NUM_STACKED_FRAMES)\n",
    "        return (np.rollaxis(batch_pre_states, 1, 4),\n",
    "                batch_actions,\n",
    "                batch_rewards,\n",
    "                np.rollaxis(batch_pos_states, 1, 4),\n",
    "                batch_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    '''\n",
    "    Creates a neural network to approximate Q values.\n",
    "    \n",
    "    Args:\n",
    "        name: scope name\n",
    "        trainable: Indicates if the graph should contain train\n",
    "        operations or not, e.g., the target network don't need \n",
    "        to be trainable\n",
    "        learning_rate: The step size used by the optimizer\n",
    "        clip_grads: Indicates if gradients should be clipped in\n",
    "        a (-1, 1) range. Prevents big changes to the network\n",
    "        \n",
    "    Returns:\n",
    "        The values of actions from state.\n",
    "    '''\n",
    "    def __init__(self, name, trainable, learning_rate=None, clip_grads=None):\n",
    "        with tf.variable_scope(name):\n",
    "            # Create placeholders\n",
    "            self.states = tf.placeholder(name='states',\n",
    "                                         shape=(None, 84, 84, NUM_STACKED_FRAMES),\n",
    "                                         dtype=tf.float32)\n",
    "            # TD targets\n",
    "            self.targets = tf.placeholder(name='targets',\n",
    "                                          shape=(None),\n",
    "                                          dtype=tf.float32)\n",
    "            # Picked actions\n",
    "            self.actions = tf.placeholder(name='actions',\n",
    "                                          shape=(None),\n",
    "                                          dtype=tf.int32)\n",
    "            \n",
    "            # Convolutional layers\n",
    "            with slim.arg_scope([slim.conv2d],\n",
    "                                weights_initializer=xavier_initializer_conv2d()):\n",
    "                self.conv = slim.stack(self.states, slim.conv2d, [\n",
    "                        (32, (8, 8), 4),\n",
    "                        (64, (4, 4), 2),\n",
    "                        (64, (3, 3), 1)\n",
    "                    ])            \n",
    "            # Fully connected layer\n",
    "            self.fc = slim.fully_connected(slim.flatten(self.conv), 512)            \n",
    "            # Output layer\n",
    "            self.values = slim.fully_connected(inputs=self.fc,\n",
    "                                               num_outputs=NUM_ACTIONS,\n",
    "                                               activation_fn=None)\n",
    "            \n",
    "            # Add training operations\n",
    "            if trainable:\n",
    "                batch_size = tf.shape(self.states)[0]\n",
    "                # Select the ids of picked actions\n",
    "                # action_ids = (i_batch * NUM_ACTIONS) + action\n",
    "                action_ids = tf.range(batch_size) * tf.shape(self.values)[1] + self.actions\n",
    "                # Only use the value of picked action\n",
    "                picked_actions_value = tf.gather(tf.reshape(self.values, [-1]),\n",
    "                                                 action_ids)\n",
    "                # Use mean squared error\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.targets,\n",
    "                                                                 picked_actions_value))\n",
    "                # Compute and apply gradients\n",
    "                opt = tf.train.AdamOptimizer(learning_rate)\n",
    "#                opt = tf.train.RMSPropOptimizer(learning_rate, 0.99, 0.0, 1e-6)\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, name)\n",
    "                self.grads_and_vars = opt.compute_gradients(self.loss, local_vars)\n",
    "                if clip_grads:\n",
    "                    self.grads_and_vars = [(tf.clip_by_value(grad, -1, 1), var)\n",
    "                                           for grad, var in self.grads_and_vars]\n",
    "                # Create or use an existing global step\n",
    "                self.global_step = slim.get_or_create_global_step()\n",
    "                self.train_op = opt.apply_gradients(self.grads_and_vars,\n",
    "                                                    self.global_step)\n",
    "                \n",
    "    def predict(self, sess, states):\n",
    "        return sess.run(self.values, feed_dict={self.states: states})\n",
    "    \n",
    "    def update(self, sess, states, actions, targets):\n",
    "        feed_dict = {self.states: states,\n",
    "                     self.actions: actions,\n",
    "                     self.targets: targets}\n",
    "        sess.run(self.train_op, feed_dict=feed_dict)\n",
    "        \n",
    "    def create_summary(self, sess, log_dir):\n",
    "        ''' \n",
    "        Create summary operations for visualization with tensorboard \n",
    "        \n",
    "        Returns:\n",
    "            A function from writing the summary\n",
    "        '''\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)            \n",
    "        self.writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "        \n",
    "        # Create placeholders to track some statistics\n",
    "        self.episode_reward = tf.placeholder(name='episode_reward',\n",
    "                                             shape=(),\n",
    "                                             dtype=tf.float32)\n",
    "        self.episode_length = tf.placeholder(name='episode_length',\n",
    "                                             shape=(),\n",
    "                                             dtype=tf.float32)\n",
    "        \n",
    "        # Add summary operations\n",
    "        tf.summary.histogram('convolution', self.conv)\n",
    "        tf.summary.histogram('last_hidden', self.fc)\n",
    "        tf.summary.histogram('q_values', self.values)\n",
    "        tf.summary.scalar('max_q_value', tf.reduce_max(self.values))\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('reward', self.episode_reward)\n",
    "        tf.summary.scalar('episode_length', self.episode_length)\n",
    "        \n",
    "        # Merge all summaries\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "        def summary_writer(states, actions, targets, ep_reward, ep_length):\n",
    "            feed_dict = {self.states: states,\n",
    "                         self.actions: actions,\n",
    "                         self.targets: targets,\n",
    "                         self.episode_reward: ep_reward,\n",
    "                         self.episode_length: ep_length}\n",
    "            summary, step = sess.run([self.merged, self.global_step],\n",
    "                                     feed_dict=feed_dict)\n",
    "            self.writer.add_summary(summary, step)\n",
    "            \n",
    "        return summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_vars(sess, from_scope, to_scope):\n",
    "    '''\n",
    "    Create operations to copy variables (weights) between two graphs\n",
    "    \n",
    "    Args:\n",
    "        sess: The current tensorflow session\n",
    "        from_scope: name of graph to copy varibles from\n",
    "        to_scope: name of graph to copy varibles to\n",
    "    '''\n",
    "    # Get variables within defined scope\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "    # Create operations that copy the variables\n",
    "    op_holder = [to_var.assign(from_var) for from_var, to_var in zip(from_vars, to_vars)]\n",
    "    \n",
    "    def run_op():\n",
    "        # Runs the operation\n",
    "        sess.run(op_holder)\n",
    "        \n",
    "    return run_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epolicy(action_values, epsilon):\n",
    "    ''' \n",
    "    Creates action probabilities based on a epsilon-greedy policy\n",
    "    '''\n",
    "    action_probs = (np.ones(NUM_ACTIONS) * epsilon) / NUM_ACTIONS\n",
    "    best_action = np.argmax(np.squeeze(action_values))\n",
    "    action_probs[best_action] += (1 - epsilon)\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_updates(learning_rate=3e-4):\n",
    "    '''\n",
    "    Test if weights updates are affecting mostly the chosen action\n",
    "    '''\n",
    "    tf.reset_default_graph()\n",
    "    test_net = QNetwork(name='test', learning_rate=learning_rate,\n",
    "                        clip_grads=True, trainable=True)\n",
    "    # Generates random states\n",
    "    test_states = np.random.random((100, 84, 84, 4))\n",
    "    # Generate fake TD targets\n",
    "    test_targets = 10 * np.ones(100)\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        test_actions = action * np.ones(100)\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            # Compare Q values before and after update\n",
    "            old_val = test_net.predict(sess, test_states)\n",
    "            test_net.update(sess, test_states, test_actions, test_targets)\n",
    "            new_val = test_net.predict(sess, test_states)\n",
    "\n",
    "        # New values should be closer to target\n",
    "        print('Action {} value should increase:'.format(action), end=' ')        \n",
    "        print(np.mean(new_val - old_val, axis=0))\n",
    "        # TODO: Should use assert here? \n",
    "        assert np.mean(new_val - old_val) > 0, 'Wrong weights updates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_copy():\n",
    "    '''\n",
    "    Test the copy of variables from one graph to another\n",
    "    Compare the predictions before and after update,\n",
    "    change main graph weigths then compare predictions again\n",
    "    '''\n",
    "    # Create graphs\n",
    "    tf.reset_default_graph()\n",
    "    net_main = QNetwork(name='main', learning_rate=3e-4, clip_grads=True, trainable=True)\n",
    "    net_target = QNetwork(name='target', trainable=False)\n",
    "    # Create a random state\n",
    "    test_state = np.random.random((1, 84, 84, 4))\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        update_target = copy_vars(sess, 'main', 'target')\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))\n",
    "        print('Copying variables...')\n",
    "        update_target()\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))\n",
    "        print('Updating main network...')\n",
    "        net_main.update(sess, test_state, [0], [100])\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))\n",
    "        print('Copying variables...')\n",
    "        update_target()\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0 value should increase: [ 1.14318228  0.00145897]\n",
      "Action 1 value should increase: [-0.00912934  1.10451889]\n"
     ]
    }
   ],
   "source": [
    "test_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.21061181  0.09048082]]\n",
      "[[-0.13393851 -0.07399513]]\n",
      "Copying variables...\n",
      "[[-0.21061181  0.09048082]]\n",
      "[[-0.21061181  0.09048082]]\n",
      "Updating main network...\n",
      "[[ 0.80587202  0.05765251]]\n",
      "[[-0.21061181  0.09048082]]\n",
      "Copying variables...\n",
      "[[ 0.80587202  0.05765251]]\n",
      "[[ 0.80587202  0.05765251]]\n"
     ]
    }
   ],
   "source": [
    "test_copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_capacity = 500000\n",
    "min_replays = 50000\n",
    "stop_exploration = 2500\n",
    "num_episodes = 5000\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.1\n",
    "batch_size = 32\n",
    "discount_factor = 0.99\n",
    "update_target_frequency = 10000\n",
    "save_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "Started training...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Episode 0/10 | Episode reward: -21.0Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Episode 1/10 | Episode reward: -21.0Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Episode 2/10 | Episode reward: -21.0Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Updating target network...\n",
      "Episode 3/10 | Episode reward: -21.0"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1cc1afca15f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mb_td_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_rewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_xor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_next_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Update main weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mnet_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_td_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Update target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-18e15fade0cf>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, sess, states, actions, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                      self.targets: targets}\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1002\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creates networks\n",
    "tf.reset_default_graph()\n",
    "net_main = QNetwork(name='main', trainable=True,\n",
    "                    learning_rate=3e-4, clip_grads=True)\n",
    "net_target = QNetwork(name='target', trainable=False)\n",
    "\n",
    "# Create saving directory\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'graph.ckpt')\n",
    "    \n",
    "# Populate replay memory with random agent\n",
    "print('Populating replay memory...')\n",
    "replays = ReplayMemory(replay_capacity)\n",
    "state = env.reset()\n",
    "# Create equal probability for picking any action\n",
    "action_probs = np.ones(NUM_ACTIONS) / NUM_ACTIONS\n",
    "for _ in range(min_replays):\n",
    "    # Pick a random action\n",
    "    action = np.random.choice(np.arange(NUM_ACTIONS), p=action_probs)\n",
    "    valid_action = VALID_ACTIONS[action]\n",
    "    next_state, reward, done, _ = env.step(valid_action)    \n",
    "    replays.add_replay(preprocess(state), action, reward, done)\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state = next_state\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Update target network\n",
    "    update_target = copy_vars(sess, 'main', 'target')    \n",
    "    update_target()    \n",
    "    # Reload last checkpoint if it exists\n",
    "    saver = tf.train.Saver()\n",
    "    last_checkpoint = tf.train.latest_checkpoint(save_dir)\n",
    "    if last_checkpoint:\n",
    "        saver.restore(sess, last_checkpoint)\n",
    "    # Create summary writer\n",
    "    write_summary = net_main.create_summary(sess, 'summaries')    \n",
    "    \n",
    "    # Calculate epsilon step size\n",
    "    epsilon_step = - np.log(epsilon_min) / stop_exploration\n",
    "    \n",
    "    steps_sum = 0\n",
    "    print('Started training...')\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Exponentially decay epsilon\n",
    "        epsilon = epsilon_min + (epsilon_max - epsilon_min) \\\n",
    "        * np.exp(-epsilon_step * i_episode)\n",
    "        \n",
    "        state = preprocess(env.reset())\n",
    "        state_buffer = np.stack([state] * 4, axis=2)   \n",
    "        ep_reward_sum = 0\n",
    "        # Repeat until episode is finished\n",
    "        for i_step in itertools.count():\n",
    "            # Select an action\n",
    "            action_values = net_main.predict(sess, state_buffer[np.newaxis, ...])\n",
    "            action_probs = epolicy(action_values, epsilon)\n",
    "            action = np.random.choice(np.arange(NUM_ACTIONS), p=action_probs)\n",
    "            valid_action = VALID_ACTIONS[action]\n",
    "            # Do the action\n",
    "            next_state, reward, done, _ = env.step(valid_action)\n",
    "            next_state = preprocess(next_state)\n",
    "            ep_reward_sum += reward\n",
    "            # Record experience\n",
    "            replays.add_replay(state, action, reward, done)\n",
    "            \n",
    "            # Sample replays to train on\n",
    "            b_states, b_actions, b_rewards, b_next_states, b_done = \\\n",
    "            replays.sample(batch_size)\n",
    "            # Perform Q learning (using target network)\n",
    "            value_next = net_target.predict(sess, b_next_states)            \n",
    "            value_next_max = np.max(value_next, axis=1)\n",
    "            b_td_targets = b_rewards + np.bitwise_xor(b_done, 1) \\\n",
    "            * (discount_factor * value_next_max)\n",
    "            # Update main weights\n",
    "            net_main.update(sess, b_states, b_actions, b_td_targets)\n",
    "            \n",
    "            # Update target network\n",
    "            steps_sum = (steps_sum + 1) % update_target_frequency\n",
    "#            print('\\rSteps sum: {}'.format(steps_sum), end='')\n",
    "            if steps_sum == 0:\n",
    "                print('\\nUpdating target network...')\n",
    "                update_target()\n",
    "            \n",
    "            # Update state\n",
    "            if done:\n",
    "                break\n",
    "            state_buffer = np.append(state_buffer[:, :, 1:],\n",
    "                                     next_state[:, :, np.newaxis],\n",
    "                                     axis=2)\n",
    "            \n",
    "        # Write summaries and save model\n",
    "        write_summary(b_states, b_actions, b_td_targets, ep_reward_sum, i_step)        \n",
    "        saver.save(sess, save_path)        \n",
    "        \n",
    "        print('\\rEpisode {}/{}'.format(i_episode, num_episodes), end=' | ')        \n",
    "        print('Episode reward: {}'.format(ep_reward_sum), end='')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
