{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.layers import xavier_initializer_conv2d\n",
    "\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-08 15:48:41,676] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "NUM_STACKED_FRAMES = 4\n",
    "VALID_ACTIONS = [0, 1, 2, 3]\n",
    "NUM_ACTIONS = len(VALID_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    ''' Converts image to grayscale and resizes to 84x84 '''\n",
    "    # Maybe try preprocess on TF? Faster on GPU?\n",
    "    return resize(rgb2gray(img), (84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    ''' \n",
    "    Creates a buffer that stores experience (replays)\n",
    "    '''\n",
    "    def __init__(self, max_replays):\n",
    "        self.max_replays = max_replays\n",
    "        # Allocate memory\n",
    "        self.imgs = np.empty((max_replays, 84, 84), dtype=np.float32)\n",
    "        self.actions = np.empty(max_replays, dtype=np.int32)\n",
    "        self.rewards = np.empty(max_replays, dtype=np.float32)\n",
    "        self.done = np.empty(max_replays, dtype=np.bool)\n",
    "        # Create \"pointers\"\n",
    "        self.current = 0 \n",
    "        self.bottom = 0\n",
    "        self.size = 0\n",
    "        \n",
    "    def add_replay(self, img, action, reward, done):\n",
    "        '''  Add experience to memory ''' \n",
    "        self.imgs[self.current] = img\n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.done[self.current] = done\n",
    "        # Update memory actual size\n",
    "        if self.size == self.max_replays:\n",
    "            self.bottom = (self.bottom + 1) % self.max_replays\n",
    "        else:\n",
    "            self.size += 1\n",
    "        # Update current memory \"pointer\"\n",
    "        self.current = (self.current + 1) % self.max_replays\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        ''' \n",
    "        Return an array containing NUM_STACKED_FRAMES sequential imgs.\n",
    "        '''\n",
    "        batch_states = np.empty((batch_size, NUM_STACKED_FRAMES + 1, 84, 84), dtype=np.float32)\n",
    "        batch_actions = np.empty(batch_size, dtype=np.int32)\n",
    "        batch_rewards = np.empty(batch_size, dtype=np.float32)\n",
    "        batch_done = np.empty(batch_size, dtype=np.bool)\n",
    "        \n",
    "        i_batch = 0\n",
    "        while i_batch < batch_size:\n",
    "            # Get a random idx to construct a single transition\n",
    "            start_idx = np.random.randint(self.bottom, self.bottom \n",
    "                                          + self.size \n",
    "                                          - NUM_STACKED_FRAMES)\n",
    "            idxs = np.arange(start_idx, start_idx + NUM_STACKED_FRAMES + 1)\n",
    "            # Only the last frame of initial state can be a finished episode\n",
    "            if np.any(self.done.take(idxs[:-2], axis=0, mode='wrap')):\n",
    "                continue\n",
    "\n",
    "            # Save transitions\n",
    "            pre_end_idx = idxs[-2]\n",
    "            batch_states[i_batch] = self.imgs.take(idxs, axis=0, mode='wrap')\n",
    "            batch_actions[i_batch] = self.actions.take(pre_end_idx, axis=0, mode='wrap')\n",
    "            batch_rewards[i_batch] = self.rewards.take(pre_end_idx, axis=0, mode='wrap')\n",
    "            batch_done[i_batch] = self.done.take(pre_end_idx, axis=0, mode='wrap')\n",
    "            i_batch += 1\n",
    "        \n",
    "        # Roll axis to be of shape: (batch_size, 84, 84, NUM_STACKED_FRAMES)\n",
    "        return (np.rollaxis(batch_states, 1, 4),\n",
    "                batch_actions,\n",
    "                batch_rewards,\n",
    "                batch_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    '''\n",
    "    Creates a neural network to approximate Q values.\n",
    "    \n",
    "    Args:\n",
    "        name: scope name\n",
    "        trainable: Indicates if the graph should contain train\n",
    "        operations or not, e.g., the target network don't need \n",
    "        to be trainable\n",
    "        learning_rate: The step size used by the optimizer\n",
    "        clip_grads: Indicates if gradients should be clipped in\n",
    "        a (-1, 1) range. Prevents big changes to the network\n",
    "        \n",
    "    Returns:\n",
    "        The values of actions from state.\n",
    "    '''\n",
    "    def __init__(self, name, trainable, learning_rate=None, clip_grads=None):\n",
    "        with tf.variable_scope(name):\n",
    "            # Create placeholders\n",
    "            self.states = tf.placeholder(name='states',\n",
    "                                         shape=(None, 84, 84, NUM_STACKED_FRAMES),\n",
    "                                         dtype=tf.float32)\n",
    "            # TD targets\n",
    "            self.targets = tf.placeholder(name='targets',\n",
    "                                          shape=(None),\n",
    "                                          dtype=tf.float32)\n",
    "            # Picked actions\n",
    "            self.actions = tf.placeholder(name='actions',\n",
    "                                          shape=(None),\n",
    "                                          dtype=tf.int32)\n",
    "            \n",
    "            # Convolutional layers\n",
    "            with slim.arg_scope([slim.conv2d],\n",
    "                                weights_initializer=xavier_initializer_conv2d()):\n",
    "                self.conv = slim.stack(self.states, slim.conv2d, [\n",
    "                        (32, (8, 8), 4),\n",
    "                        (64, (4, 4), 2),\n",
    "                        (64, (3, 3), 1)\n",
    "                    ])            \n",
    "            # Fully connected layer\n",
    "            self.fc = slim.fully_connected(slim.flatten(self.conv), 512)            \n",
    "            # Output layer\n",
    "            self.values = slim.fully_connected(inputs=self.fc,\n",
    "                                               num_outputs=NUM_ACTIONS,\n",
    "                                               activation_fn=None)\n",
    "            \n",
    "            # Add training operations\n",
    "            if trainable:\n",
    "                batch_size = tf.shape(self.states)[0]\n",
    "                # Select the ids of picked actions\n",
    "                # action_ids = (i_batch * NUM_ACTIONS) + action\n",
    "                action_ids = tf.range(batch_size) * tf.shape(self.values)[1] + self.actions\n",
    "                # Only use the value of picked action\n",
    "                picked_actions_value = tf.gather(tf.reshape(self.values, [-1]),\n",
    "                                                 action_ids)\n",
    "                # Use mean squared error\n",
    "                self.loss = tf.reduce_mean(tf.squared_difference(self.targets,\n",
    "                                                                 picked_actions_value))\n",
    "                # Compute and apply gradients\n",
    "#                opt = tf.train.AdamOptimizer(learning_rate)\n",
    "                opt = tf.train.RMSPropOptimizer(learning_rate, 0.99, 0.0, 1e-6)\n",
    "                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, name)\n",
    "                self.grads_and_vars = opt.compute_gradients(self.loss, local_vars)\n",
    "                if clip_grads:\n",
    "                    self.grads_and_vars = [(tf.clip_by_value(grad, -1, 1), var)\n",
    "                                           for grad, var in self.grads_and_vars]\n",
    "                # Create or use an existing global step\n",
    "                self.global_step = slim.get_or_create_global_step()\n",
    "                self.train_op = opt.apply_gradients(self.grads_and_vars,\n",
    "                                                    self.global_step)\n",
    "                \n",
    "    def predict(self, sess, states):\n",
    "        return sess.run(self.values, feed_dict={self.states: states})\n",
    "    \n",
    "    def update(self, sess, states, actions, targets):\n",
    "        feed_dict = {self.states: states,\n",
    "                     self.actions: actions,\n",
    "                     self.targets: targets}\n",
    "        sess.run(self.train_op, feed_dict=feed_dict)\n",
    "        \n",
    "    def create_summary(self, sess, log_dir):\n",
    "        ''' \n",
    "        Create summary operations for visualization with tensorboard \n",
    "        \n",
    "        Returns:\n",
    "            A function for writing the summary\n",
    "        '''\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)            \n",
    "        self.writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "        \n",
    "        # Create placeholders to track some statistics\n",
    "        self.episode_reward = tf.placeholder(name='episode_reward',\n",
    "                                             shape=(),\n",
    "                                             dtype=tf.float32)\n",
    "        self.episode_length = tf.placeholder(name='episode_length',\n",
    "                                             shape=(),\n",
    "                                             dtype=tf.float32)\n",
    "        self.epsilon = tf.placeholder(name='epsilon',\n",
    "                                      shape=(),\n",
    "                                      dtype=tf.float32)\n",
    "        \n",
    "        # Add summary operations\n",
    "        tf.summary.histogram('convolution', self.conv)\n",
    "        tf.summary.histogram('last_hidden', self.fc)\n",
    "        tf.summary.histogram('q_values', self.values)\n",
    "        tf.summary.scalar('max_q_value', tf.reduce_max(self.values))\n",
    "        tf.summary.scalar('avarege_q_value', tf.reduce_mean(self.values))\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('reward', self.episode_reward)\n",
    "        tf.summary.scalar('episode_length', self.episode_length)\n",
    "        tf.summary.scalar('exploration_rate', self.epsilon)\n",
    "        # Merge all summaries\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        \n",
    "        def summary_writer(states, actions, targets, ep_reward, ep_length, epsilon):\n",
    "            feed_dict = {self.states: states,\n",
    "                         self.actions: actions,\n",
    "                         self.targets: targets,\n",
    "                         self.episode_reward: ep_reward,\n",
    "                         self.episode_length: ep_length,\n",
    "                         self.epsilon: epsilon}\n",
    "            summary, step = sess.run([self.merged, self.global_step],\n",
    "                                     feed_dict=feed_dict)\n",
    "            self.writer.add_summary(summary, step)\n",
    "            \n",
    "        return summary_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copy_vars(sess, from_scope, to_scope):\n",
    "    '''\n",
    "    Create operations to copy variables (weights) between two graphs\n",
    "    \n",
    "    Args:\n",
    "        sess: The current tensorflow session\n",
    "        from_scope: name of graph to copy varibles from\n",
    "        to_scope: name of graph to copy varibles to\n",
    "    '''\n",
    "    # Get variables within defined scope\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "    # Create operations that copy the variables\n",
    "    op_holder = [to_var.assign(from_var) for from_var, to_var in zip(from_vars, to_vars)]\n",
    "    \n",
    "    def run_op():\n",
    "        # Runs the operation\n",
    "        sess.run(op_holder)\n",
    "        \n",
    "    return run_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epolicy(action_values, epsilon):\n",
    "    ''' \n",
    "    Creates action probabilities based on a epsilon-greedy policy\n",
    "    '''\n",
    "    action_probs = (np.ones(NUM_ACTIONS) * epsilon) / NUM_ACTIONS\n",
    "    best_action = np.argmax(np.squeeze(action_values))\n",
    "    action_probs[best_action] += (1 - epsilon)\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_updates(learning_rate=3e-4):\n",
    "    '''\n",
    "    Test if weights updates are affecting mostly the chosen action\n",
    "    '''\n",
    "    tf.reset_default_graph()\n",
    "    test_net = QNetwork(name='test', learning_rate=learning_rate,\n",
    "                        clip_grads=True, trainable=True)\n",
    "    # Generates random states\n",
    "    test_states = np.random.random((100, 84, 84, 4))\n",
    "    # Generate fake TD targets\n",
    "    test_targets = 10 * np.ones(100)\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        test_actions = action * np.ones(100)\n",
    "        with tf.Session() as sess:\n",
    "            tf.global_variables_initializer().run()\n",
    "            # Compare Q values before and after update\n",
    "            old_val = test_net.predict(sess, test_states)\n",
    "            test_net.update(sess, test_states, test_actions, test_targets)\n",
    "            new_val = test_net.predict(sess, test_states)\n",
    "\n",
    "        # New values should be closer to target\n",
    "        print('Action {} value should increase:'.format(action), end=' ')        \n",
    "        print(np.mean(new_val - old_val, axis=0))\n",
    "        # TODO: Should use assert here? \n",
    "        assert np.mean(new_val - old_val) > 0, 'Wrong weights updates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_copy():\n",
    "    '''\n",
    "    Test the copy of variables from one graph to another\n",
    "    Compare the predictions before and after update,\n",
    "    change main graph weigths then compare predictions again\n",
    "    '''\n",
    "    # Create graphs\n",
    "    tf.reset_default_graph()\n",
    "    net_main = QNetwork(name='main', learning_rate=3e-4, clip_grads=True, trainable=True)\n",
    "    net_target = QNetwork(name='target', trainable=False)\n",
    "    # Create a random state\n",
    "    test_state = np.random.random((1, 84, 84, 4))\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        update_target = copy_vars(sess, 'main', 'target')\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))\n",
    "        print('Copying variables...')\n",
    "        update_target()\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))\n",
    "        print('Updating main network...')\n",
    "        net_main.update(sess, test_state, [0], [100])\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))\n",
    "        print('Copying variables...')\n",
    "        update_target()\n",
    "        print(net_main.predict(sess, test_state))\n",
    "        print(net_target.predict(sess, test_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0 value should increase: [ 0.14992034 -0.02233231  0.00964264 -0.00309687]\n",
      "Action 1 value should increase: [ 0.01275891  0.19369437  0.0083417   0.01769515]\n",
      "Action 2 value should increase: [-0.00087386  0.02050029  0.13262562  0.01031024]\n",
      "Action 3 value should increase: [-0.00996291 -0.01605413  0.00687522  0.13047385]\n"
     ]
    }
   ],
   "source": [
    "test_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.15641092  0.02883101  0.12117202 -0.09916534]]\n",
      "[[  1.22871250e-04  -8.24126303e-02   1.25503931e-02   3.33613873e-01]]\n",
      "Copying variables...\n",
      "[[ 0.15641092  0.02883101  0.12117202 -0.09916534]]\n",
      "[[ 0.15641092  0.02883101  0.12117202 -0.09916534]]\n",
      "Updating main network...\n",
      "[[ 1.16144538  0.07992022  0.25706494 -0.15212522]]\n",
      "[[ 0.15641092  0.02883101  0.12117202 -0.09916534]]\n",
      "Copying variables...\n",
      "[[ 1.16144538  0.07992022  0.25706494 -0.15212522]]\n",
      "[[ 1.16144538  0.07992022  0.25706494 -0.15212522]]\n"
     ]
    }
   ],
   "source": [
    "test_copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_capacity = 100000\n",
    "min_replays = 10000\n",
    "stop_exploration = 25000\n",
    "num_episodes = 5000\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.1\n",
    "batch_size = 32\n",
    "discount_factor = 0.99\n",
    "update_target_frequency = 10000\n",
    "save_dir = 'checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "Started training...\n",
      "Episode 39/5000 | Episode reward: 0.0 (Estimated time left: 175132.36749649048)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-08 16:13:17,057] Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-c2708ebdf4a7>\", line 73, in <module>\n",
      "    value_next = net_target.predict(sess, b_states[..., 1:])\n",
      "  File \"<ipython-input-5-d9c7fa5fb6db>\", line 73, in predict\n",
      "    return sess.run(self.values, feed_dict={self.states: states})\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 766, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n",
      "    feed_dict_string, options, run_metadata)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n",
      "    target_list, options, run_metadata)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1021, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1003, in _run_fn\n",
      "    status, run_metadata)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/inspect.py\", line 1453, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/inspect.py\", line 1410, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/inspect.py\", line 672, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/quazar/anaconda3/lib/python3.5/inspect.py\", line 708, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Creates networks\n",
    "tf.reset_default_graph()\n",
    "net_main = QNetwork(name='main', trainable=True,\n",
    "                    learning_rate=3e-4, clip_grads=True)\n",
    "net_target = QNetwork(name='target', trainable=False)\n",
    "\n",
    "# Create saving directory\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "save_path = os.path.join(save_dir, 'graph.ckpt')\n",
    "    \n",
    "# Populate replay memory with random agent\n",
    "print('Populating replay memory...')\n",
    "replays = ReplayMemory(replay_capacity)\n",
    "state = env.reset()\n",
    "# Create equal probability for picking any action\n",
    "action_probs = np.ones(NUM_ACTIONS) / NUM_ACTIONS\n",
    "for _ in range(min_replays):\n",
    "    # Pick a random action\n",
    "    action = np.random.choice(np.arange(NUM_ACTIONS), p=action_probs)\n",
    "    valid_action = VALID_ACTIONS[action]\n",
    "    next_state, reward, done, _ = env.step(valid_action)    \n",
    "    replays.add_replay(preprocess(state), action, reward, done)\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "    state = next_state\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Update target network\n",
    "    update_target = copy_vars(sess, 'main', 'target')    \n",
    "    update_target()    \n",
    "    # Reload last checkpoint if it exists\n",
    "    saver = tf.train.Saver()\n",
    "    last_checkpoint = tf.train.latest_checkpoint(save_dir)\n",
    "    if last_checkpoint:\n",
    "        saver.restore(sess, last_checkpoint)\n",
    "    # Create summary writer\n",
    "    write_summary = net_main.create_summary(sess, 'summaries/breakout1')    \n",
    "    \n",
    "    # Calculate epsilon step size\n",
    "    epsilon_step = - np.log(epsilon_min) / stop_exploration\n",
    "    \n",
    "    steps_sum = 0\n",
    "    print('Started training...')\n",
    "    for i_episode in range(num_episodes + 1):\n",
    "        ep_start = time.time()\n",
    "        \n",
    "        state = preprocess(env.reset())\n",
    "        state_buffer = np.stack([state] * 4, axis=2)   \n",
    "        ep_reward_sum = 0\n",
    "        # Repeat until episode is finished\n",
    "        for i_step in itertools.count():\n",
    "            # Exponentially decay epsilon\n",
    "            epsilon = epsilon_min + (epsilon_max - epsilon_min) \\\n",
    "            * np.exp(-epsilon_step * i_episode)\n",
    "            # Select an action\n",
    "            action_values = net_main.predict(sess, state_buffer[np.newaxis, ...])\n",
    "            action_probs = epolicy(action_values, epsilon)\n",
    "            action = np.random.choice(np.arange(NUM_ACTIONS), p=action_probs)\n",
    "            valid_action = VALID_ACTIONS[action]\n",
    "            # Do the action\n",
    "            next_state, reward, done, _ = env.step(valid_action)\n",
    "            next_state = preprocess(next_state)\n",
    "            ep_reward_sum += reward\n",
    "            # Record experience\n",
    "            replays.add_replay(state, action, reward, done)\n",
    "            \n",
    "            # Sample replays to train on\n",
    "            b_states, b_actions, b_rewards, b_done = \\\n",
    "            replays.sample(batch_size)\n",
    "            # Perform Q learning (using target network)\n",
    "            value_next = net_target.predict(sess, b_states[..., 1:])            \n",
    "            value_next_max = np.max(value_next, axis=1)\n",
    "            b_td_targets = b_rewards + (1 - b_done) \\\n",
    "            * (discount_factor * value_next_max)\n",
    "            # Update main weights\n",
    "            net_main.update(sess, b_states[..., :-1], b_actions, b_td_targets)\n",
    "            \n",
    "            # Update target network\n",
    "            steps_sum += 1\n",
    "#            print('\\rSteps sum: {}'.format(steps_sum), end='')\n",
    "            if steps_sum % update_target_frequency == 0:\n",
    "                print('\\nUpdating target network...')\n",
    "                update_target()\n",
    "            \n",
    "            # Update state\n",
    "            if done:\n",
    "                break\n",
    "            state_buffer = np.append(state_buffer[:, :, 1:],\n",
    "                                     next_state[:, :, np.newaxis],\n",
    "                                     axis=2)\n",
    "            \n",
    "        # Write summaries and save model\n",
    "        write_summary(b_states[..., :-1],\n",
    "                      b_actions,\n",
    "                      b_td_targets,\n",
    "                      ep_reward_sum,\n",
    "                      i_step,\n",
    "                      epsilon)        \n",
    "        if i_episode % 50:\n",
    "            saver.save(sess, save_path)        \n",
    "        # Episode time length\n",
    "        ep_time = time.time() - ep_start\n",
    "        \n",
    "        print('\\rEpisode {}/{}'.format(i_episode, num_episodes), end=' | ')        \n",
    "        print('Episode reward: {}'.format(ep_reward_sum), end=' ')\n",
    "        print('Episode time length: {}'.format(ep_time), end='')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
